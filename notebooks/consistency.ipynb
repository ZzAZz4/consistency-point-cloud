{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class PointConsistencyModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module, sigma_data=0.5, sigma_min=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sigma_data = sigma_data\n",
    "        self.epsilon = sigma_min\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor, batch: Tensor | None=None) -> Tensor:\n",
    "        f = self.model(x, t, batch)\n",
    "        print(f.shape)\n",
    "        c_out = self.c_out(t)\n",
    "        c_skip = self.c_skip(t)\n",
    "        out = c_out[batch].unsqueeze(-1) * f \n",
    "        out += c_skip[batch].unsqueeze(-1) * x\n",
    "        return out\n",
    "\n",
    "    def c_skip(self, t: Tensor) -> Tensor:\n",
    "        return torch.div(self.sigma_data**2, (t - self.epsilon)**2 + self.sigma_data**2)\n",
    "    \n",
    "    def c_out(self, t: Tensor) -> Tensor:\n",
    "        return torch.div(self.sigma_data * (t - self.epsilon), torch.sqrt(self.sigma_data**2 + t**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import typing as ty\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PointConsistencySettings:\n",
    "    training_iterations: int\n",
    "    target_disc_steps: tuple[int, int] = (1, 150)\n",
    "    initial_ema_decay_rate: float = 0.95\n",
    "    initial_timesteps: int = 2\n",
    "    sigma_range: tuple[float, float] = (0.002, 1.0)\n",
    "    rho: float = 7.0\n",
    "\n",
    "\n",
    "class PointConsistencyTraining(nn.Module):\n",
    "    def __init__(self, settings: PointConsistencySettings):\n",
    "        super().__init__()\n",
    "        self.conf = settings\n",
    "        \n",
    "    def step_schedule_n(self, k: float) -> int:\n",
    "        s, K = self.conf.target_disc_steps, self.conf.training_iterations\n",
    "\n",
    "        num_timesteps = (s[1] + 1)**2 - s[0]**2\n",
    "        num_timesteps = k * num_timesteps / K\n",
    "        num_timesteps = num_timesteps + s[0]**2\n",
    "        num_timesteps = math.sqrt(num_timesteps)\n",
    "        num_timesteps = math.ceil(-1. + num_timesteps)\n",
    "        return 1 + num_timesteps\n",
    "\n",
    "    def ema_decay_rate_schedule_mu(self, n_k: int) -> float:\n",
    "        s, mu_0 = self.conf.target_disc_steps, self.conf.initial_ema_decay_rate\n",
    "        \n",
    "        return math.exp(s[0] * math.log(mu_0) / float(n_k))\n",
    "\n",
    "    def karras_schedule_t(self, n_k: int, device: torch.device | None = None) -> Tensor:\n",
    "        (eps, T), rho = self.conf.sigma_range, self.conf.rho\n",
    "\n",
    "        rho_inv = 1.0 / rho\n",
    "        steps = torch.arange(n_k, device=device) / max(n_k - 1, 1)\n",
    "        sigmas = eps**rho_inv + steps * (T**rho_inv - eps**rho_inv)\n",
    "        sigmas = sigmas**rho\n",
    "        return sigmas\n",
    "\n",
    "    def ema_decay_rate_schedule(self, num_timesteps: int) -> float:\n",
    "        return math.exp(\n",
    "            (self.conf.initial_timesteps * math.log(self.conf.initial_ema_decay_rate)) / num_timesteps\n",
    "        )\n",
    "\n",
    "    def train_step(\n",
    "        self, \n",
    "        iteration: int, \n",
    "        x: Tensor, batch: Tensor, \n",
    "        model: PointConsistencyModel, \n",
    "        ema_model: PointConsistencyModel\n",
    "    ):\n",
    "        num_timesteps = self.step_schedule_n(iteration)\n",
    "        sigmas = self.karras_schedule_t(num_timesteps, device=x.device)\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        timesteps = torch.randint(0, num_timesteps - 1, (int(batch.max().item()) + 1, ), device=x.device)\n",
    "        current_sigmas = sigmas[timesteps]\n",
    "        next_sigmas = sigmas[timesteps + 1]\n",
    "\n",
    "        next_x = x + (noise * next_sigmas[batch].unsqueeze(-1))\n",
    "        next_x = model(next_x, next_sigmas, batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            current_x = x + (x * current_sigmas[batch].unsqueeze(-1))\n",
    "            current_x = ema_model(current_x, current_sigmas, batch)\n",
    "\n",
    "        return next_x, current_x\n",
    "\n",
    "    def after_train_step(self, iteration: int, model: PointConsistencyModel, ema_model: PointConsistencyModel):\n",
    "        num_timesteps = self.step_schedule_n(iteration)\n",
    "        ema_decay_rate = self.ema_decay_rate_schedule(num_timesteps)\n",
    "        self._update_ema_weights(\n",
    "            ema_model.parameters(), model.parameters(), ema_decay_rate\n",
    "        )\n",
    "        return ema_model\n",
    "\n",
    "    def _update_ema_weights(\n",
    "        self,\n",
    "        ema_weight_iter: ty.Iterator[Tensor],\n",
    "        online_weight_iter: ty.Iterator[Tensor],\n",
    "        ema_decay_rate: float,\n",
    "    ) -> None:\n",
    "        for ema_weight, online_weight in zip(ema_weight_iter, online_weight_iter):\n",
    "            if ema_weight.data is None:\n",
    "                ema_weight.data.copy_(online_weight.data)\n",
    "            else:\n",
    "                ema_weight.data.lerp_(online_weight.data, 1.0 - ema_decay_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "from models.model import Model\n",
    "\n",
    "path = \"data/ShapeNet\"\n",
    "category = 'Airplane' \n",
    "transform = T.Compose([\n",
    "    T.NormalizeRotation(),\n",
    "    T.FixedPoints(1024),\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.NormalizeRotation(),\n",
    "    T.FixedPoints(1024),\n",
    "])\n",
    "pre_transform = T.NormalizeScale()\n",
    "train_dataset = ShapeNet(path, category, split='trainval', transform=transform, pre_transform=pre_transform)\n",
    "test_dataset = ShapeNet(path, category, split='test', transform=test_transform, pre_transform=pre_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "class CDLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return chamfer_distance(pred, target)[0]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PointConsistencyModel(Model())\n",
    "ema = PointConsistencyModel(Model())\n",
    "ema.load_state_dict(model.state_dict())\n",
    "ema.eval()\n",
    "\n",
    "model = model.to(device)\n",
    "ema = ema.to(device)\n",
    "\n",
    "ct = PointConsistencyTraining(PointConsistencySettings(100_000))\n",
    "loss = CDLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "for i, data in enumerate(train_loader, 1):\n",
    "    optimizer.zero_grad()\n",
    "    data = data.to(device)\n",
    "\n",
    "    next, cur = ct.train_step(i, data.pos, data.batch, model, ema)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_points(pos, c=None):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    ax.axes.zaxis.set_ticklabels([]) # type: ignore\n",
    "    ax.scatter(pos[:, 0], pos[:, 1], pos[:, 2], c='blue' if c is None else c, s=3)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.loss import chamfer_distance\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch import nn\n",
    "\n",
    "class CDLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target, batch):\n",
    "        pred, target = to_dense_batch(pred, batch)[0], to_dense_batch(target, batch)[0]\n",
    "        return chamfer_distance(pred, target)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "path = \"data/ShapeNet\"\n",
    "category = 'Airplane' \n",
    "transform = T.Compose([\n",
    "    T.NormalizeRotation(),\n",
    "    T.FixedPoints(1024),\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.NormalizeRotation(),\n",
    "    T.FixedPoints(1024),\n",
    "])\n",
    "pre_transform = T.NormalizeScale()\n",
    "train_dataset = ShapeNet(path, category, split='trainval', transform=transform, pre_transform=pre_transform)\n",
    "test_dataset = ShapeNet(path, category, split='test', transform=test_transform, pre_transform=pre_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "settings = PointConsistencySettings(training_iterations=100_000, sigma_range=(0.02, 20))\n",
    "ct = PointConsistencyTraining(settings)\n",
    "\n",
    "model = PointConsistencyModel(Model(), sigma_data=0.5, sigma_min=settings.sigma_range[0])\n",
    "ema_model = PointConsistencyModel(Model(), sigma_data=0.5, sigma_min=settings.sigma_range[0])\n",
    "ema_model.load_state_dict(model.state_dict())\n",
    "loss = CDLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, ema_model = model.to(device), ema_model.to(device)\n",
    "\n",
    "k = 1\n",
    "for epoch in range(2, settings.training_iterations // len(train_loader) + 1):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        next_x, cur_x = ct.train_step(k, data.pos, data.batch, model, ema_model)\n",
    "\n",
    "        loss_val = loss(next_x, cur_x)\n",
    "        loss_val.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        ema_model = ct.after_train_step(k, model, ema_model)\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Iteration: {k}, Loss: {loss_val.item()}\")\n",
    "\n",
    "        if k % 10 == 1:\n",
    "            first_next_x = to_dense_batch(next_x, data.batch)[0][0]\n",
    "            visualize_points(first_next_x.detach().cpu().numpy())\n",
    "\n",
    "            first_cur_x = to_dense_batch(cur_x, data.batch)[0][0]\n",
    "            visualize_points(first_cur_x.detach().cpu().numpy())\n",
    "        \n",
    "        k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "path = \"data/ShapeNet\"\n",
    "category = 'Airplane' \n",
    "transform = T.Compose([\n",
    "    T.NormalizeRotation(),\n",
    "    T.FixedPoints(1024),\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.FixedPoints(1024),\n",
    "])\n",
    "pre_transform = T.NormalizeScale()\n",
    "train_dataset = ShapeNet(path, category, split='trainval', transform=transform, pre_transform=pre_transform)\n",
    "test_dataset = ShapeNet(path, category, split='test', transform=test_transform, pre_transform=pre_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "# Helper functions for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_mesh(pos, face):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    ax.axes.zaxis.set_ticklabels([]) # type: ignore\n",
    "    ax.plot_trisurf(pos[:, 0], pos[:, 1], pos[:, 2], triangles=face.t(), antialiased=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_points(pos, c=None):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    ax.axes.zaxis.set_ticklabels([]) # type: ignore\n",
    "    ax.scatter(pos[:, 0], pos[:, 2], pos[:, 1], c=pos[:, 1], s=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.loss import chamfer_distance\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "\n",
    "def chamfer_loss(x, y, batch):\n",
    "    x, y = to_dense_batch(x, batch)[0], to_dense_batch(y, batch)[0]\n",
    "    return chamfer_distance(x, y)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chamfer_loss(train_dataset[4].pos, test_dataset[4].pos, train_dataset[8].batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        data_morph = data.clone()\n",
    "        data_morph.pos += torch.randn_like(data.pos) * 0.2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data_morph)\n",
    "        loss = chamfer_loss(out, data.pos, data.batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss/10:.4f} ')\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        data_morph = data.clone()\n",
    "        data_morph.pos += torch.randn_like(data.pos) * 0.2\n",
    "\n",
    "        out = model(data_morph)\n",
    "        loss = chamfer_loss(out, data.pos, data.batch)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i == 0:\n",
    "            data_pos = to_dense_batch(data.pos, data.batch)[0][0]\n",
    "            visualize_points(data_pos.detach().cpu())   \n",
    "            morphed_pos = to_dense_batch(data_morph.pos, data.batch)[0][0]\n",
    "            visualize_points(morphed_pos.detach().cpu())\n",
    "            new_pos = to_dense_batch(out, data.batch)[0][0]\n",
    "            visualize_points(new_pos.detach().cpu())\n",
    "\n",
    "    return total_loss / len(test_loader)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "for epochs in range(100):\n",
    "    print(f'Epoch {epochs}')\n",
    "    train()\n",
    "    test_loss = test()\n",
    "    print(f'Test loss: {test_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
