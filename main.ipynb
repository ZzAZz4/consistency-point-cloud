{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as gnn\n",
    "from torch import Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConsistencySettings:\n",
    "    training_iterations: int = 100_000\n",
    "    min_time_partitions: int = 1\n",
    "    max_time_partitions: int = 150\n",
    "    initial_ema_decay: float = 0.99\n",
    "    min_time: float = 1e-4\n",
    "    data_time: float = 1e-3\n",
    "    max_time: float = 1.0\n",
    "    rho: float = 7.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from loss.chamfer_loss import CDLoss\n",
    "from geomloss import SamplesLoss\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "class BaseDistanceFunc(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def __call__(self, lhs: Tensor, rhs: Tensor, batch: Tensor | None) -> Tensor:\n",
    "        ...\n",
    "    \n",
    "\n",
    "class ChamferDistance(BaseDistanceFunc):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.loss = CDLoss()\n",
    "        self.loss.eval()\n",
    "        \n",
    "\n",
    "    def __call__(self, lhs: Tensor, rhs: Tensor, batch: Tensor | None) -> Tensor:\n",
    "        return self.loss(lhs, rhs, batch)\n",
    "    \n",
    "\n",
    "class MSEDistance(BaseDistanceFunc):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def __call__(self, lhs: Tensor, rhs: Tensor, batch: Tensor | None) -> Tensor:\n",
    "        return self.loss(lhs, rhs)\n",
    "\n",
    "\n",
    "\n",
    "class SinkhornEMDistance(BaseDistanceFunc):\n",
    "    def __init__(self):\n",
    "        self.loss = SamplesLoss(loss=\"sinkhorn\", p=1, blur=0.01)\n",
    "        \n",
    "    def __call__(self, lhs: Tensor, rhs: Tensor, batch: Tensor | None) -> Tensor:\n",
    "        lhs, rhs = to_dense_batch(lhs, batch)[0], to_dense_batch(rhs, batch)[0]\n",
    "        return self.loss(lhs, rhs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class BaseNumTimestepsSchedule(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, iteration: int) -> int:\n",
    "        ...\n",
    "    \n",
    "\n",
    "class NumTimestepsSchedule(BaseNumTimestepsSchedule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        min_time_partitions: int,\n",
    "        max_time_partitions: int,\n",
    "        training_iterations: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target_disc_steps = (min_time_partitions, max_time_partitions)\n",
    "        self.training_iterations = training_iterations\n",
    "\n",
    "    def __call__(self, iteration: int) -> int:\n",
    "        s, K = self.target_disc_steps, self.training_iterations\n",
    "        num_timesteps = (s[1] + 1)**2 - s[0]**2\n",
    "        num_timesteps = iteration * num_timesteps / K\n",
    "        num_timesteps = num_timesteps + s[0]**2\n",
    "        num_timesteps = math.sqrt(num_timesteps)\n",
    "        num_timesteps = math.ceil(-1. + num_timesteps)\n",
    "        return 1 + num_timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseTimeSchedule(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, up_to: int, device: torch.device | None=None) -> torch.Tensor:\n",
    "        ...\n",
    "    \n",
    "    \n",
    "class KarrasTimeSchedule(BaseTimeSchedule):\n",
    "    def __init__(self, min_time: float, max_time: float, rho: float) -> None:\n",
    "        super().__init__()\n",
    "        self.sigma_range = (min_time, max_time)\n",
    "        self.rho = rho\n",
    "\n",
    "    def __call__(self, up_to: int, device: torch.device | None=None) -> torch.Tensor:\n",
    "        (eps, T), rho = self.sigma_range, self.rho\n",
    "        rho_inv = 1.0 / rho\n",
    "        steps = torch.arange(up_to, device=device) / max(up_to - 1, 1)\n",
    "        sigmas = eps**rho_inv + steps * (T**rho_inv - eps**rho_inv)\n",
    "        sigmas = sigmas**rho\n",
    "        return sigmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseEMADecay(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, iteration: int) -> float:\n",
    "        ...\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "class ExponentialDecay(BaseEMADecay):\n",
    "    def __init__(self, initial_decay: float, training_iterations: int) -> None:\n",
    "        super().__init__()\n",
    "        self.initial_decay = initial_decay\n",
    "        self.training_iterations = training_iterations\n",
    "\n",
    "    def __call__(self, iteration: int) -> float:\n",
    "        return math.exp(\n",
    "            iteration * math.log(self.initial_decay) / self.training_iterations\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseParametrization(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: Tensor, y: Tensor, t: Tensor, batch: Tensor) -> Tensor:\n",
    "        ...\n",
    "        \n",
    "\n",
    "class BaseResampler(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: Tensor, t: Tensor, batch: Tensor) -> Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class EpsilonParametrization(BaseParametrization):\n",
    "    def __init__(self, min_time: float, data_time: float) -> None:\n",
    "        super().__init__()\n",
    "        self.data_time = data_time\n",
    "        self.min_time = min_time\n",
    "\n",
    "    def skip(self, t: Tensor) -> Tensor:\n",
    "        return (self.data_time ** 2) / ((t - self.min_time) ** 2 + (self.data_time ** 2))\n",
    "\n",
    "    def out(self, t: Tensor) -> Tensor:\n",
    "        return (t - self.min_time) * self.data_time / (self.data_time**2 + t**2) ** 0.5    \n",
    "\n",
    "    def __call__(self, x: Tensor, y: Tensor, t: Tensor, batch: Tensor) -> Tensor:\n",
    "        return self.skip(t)[batch, None] * x + self.out(t)[batch, None] * y \n",
    "    \n",
    "\n",
    "class EpsilonResampler(BaseResampler):\n",
    "    def __init__(self, min_time: float) -> None:\n",
    "        super().__init__()\n",
    "        self.min_time = min_time\n",
    "\n",
    "    def __call__(self, x: Tensor, t: Tensor, batch: Tensor) -> Tensor:\n",
    "        mul = (t**2 - self.min_time**2)**0.5\n",
    "        return x + mul[batch, None] * torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class BaseConditionedModel(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor, t: Tensor, ctx: torch.Tensor, batch: Tensor) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x: Tensor, t: Tensor, ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        return super().__call__(x=x, t=t, ctx=ctx, batch=batch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class ConsistencyModel(nn.Module):\n",
    "    def __init__(self, model: BaseConditionedModel, resampler: BaseResampler, parametrization: BaseParametrization) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.resampler = resampler\n",
    "        self.parametrization = parametrization\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor | Sequence[Tensor], ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        assert not isinstance(t, int)\n",
    "        ts = (t,) if isinstance(t, Tensor) else t\n",
    "\n",
    "        x = self.wrapped_model(x=x, t=ts[0], ctx=ctx, batch=batch)\n",
    "        for t in ts[1:]:\n",
    "            x = self.resampler(x=x, t=t, batch=batch)\n",
    "            x = self.wrapped_model(x=x, t=t, ctx=ctx, batch=batch)\n",
    "        return x\n",
    "\n",
    "    def wrapped_model(self, x: Tensor, t: Tensor, ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        y = self.model(x=x, t=t, ctx=ctx, batch=batch)\n",
    "        return self.parametrization(x=x, y=y, t=t, batch=batch)\n",
    "\n",
    "    def __call__(self, x: Tensor, t: Tensor | Sequence[Tensor], ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        return super().__call__(x=x, t=t, ctx=ctx, batch=batch)\n",
    "\n",
    "\n",
    "class ConsistencyTrainer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 model: ConsistencyModel, \n",
    "                 ema: ConsistencyModel,\n",
    "                 step_schedule: BaseNumTimestepsSchedule, \n",
    "                 time_schedule: BaseTimeSchedule,\n",
    "                 ema_decay: BaseEMADecay):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.ema = ema\n",
    "\n",
    "        self.step_schedule = step_schedule\n",
    "        self.time_schedule = time_schedule\n",
    "        self.ema_decay = ema_decay\n",
    "        \n",
    "    def train_step(self, iteration: int, x: Tensor, ctx: Tensor, batch: Tensor):\n",
    "        num_timesteps = self.step_schedule(iteration)\n",
    "        times = self.time_schedule(num_timesteps, device=x.device)\n",
    "        \n",
    "        batch_size = int(batch.max()) + 1\n",
    "        time_indices = torch.randint(0, num_timesteps - 1, (batch_size,), device=x.device)\n",
    "        current_times = times[time_indices]\n",
    "        next_times = times[time_indices + 1]\n",
    "\n",
    "        z = torch.randn_like(x, device=x.device)\n",
    "        next_x = x + z * next_times[batch, None]\n",
    "        denoised = self.model(\n",
    "            x=next_x, \n",
    "            t=next_times, \n",
    "            ctx=ctx,\n",
    "            batch=batch, \n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            current_x = x + z * current_times[batch, None]\n",
    "            ema_denoised = self.ema(\n",
    "                x=current_x, \n",
    "                t=current_times, \n",
    "                ctx=ctx,\n",
    "                batch=batch, \n",
    "            )\n",
    "\n",
    "        return denoised, ema_denoised \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_emas(self, iteration: int):\n",
    "        alpha = self.ema_decay(iteration)\n",
    "        for p, ema_p in zip(self.model.parameters(), self.ema.parameters()):\n",
    "            ema_p.data = alpha * ema_p.data + (1 - alpha) * p.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch_geometric.nn import knn\n",
    "\n",
    "gt = torch.tensor([[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0]])\n",
    "batch_x = torch.tensor([0, 0, 0, 0])\n",
    "y = torch.tensor([[-1.0, 0.0]])\n",
    "assign_index = knn(gt, y, 2, batch_x)\n",
    "\n",
    "assign_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = ConsistencySettings(\n",
    "    training_iterations=100_000,\n",
    "    min_time_partitions=2,\n",
    "    max_time_partitions=150,\n",
    "    initial_ema_decay=0.95,\n",
    "    min_time=1e-4,\n",
    "    data_time=1e-3,\n",
    "    max_time=1.0,\n",
    "    rho=7.0\n",
    ")\n",
    "\n",
    "step_schedule = NumTimestepsSchedule(\n",
    "    settings.min_time_partitions,\n",
    "    settings.max_time_partitions,\n",
    "    settings.training_iterations\n",
    ")\n",
    "time_schedule = KarrasTimeSchedule(\n",
    "    settings.min_time,\n",
    "    settings.max_time,\n",
    "    settings.rho\n",
    ")\n",
    "ema_decay = ExponentialDecay(\n",
    "    settings.initial_ema_decay,\n",
    "    settings.training_iterations\n",
    ")\n",
    "parametrization = EpsilonParametrization(\n",
    "    settings.min_time,\n",
    "    settings.data_time\n",
    ")\n",
    "resampler = EpsilonResampler(\n",
    "    settings.min_time\n",
    ")\n",
    "loss_function = ChamferDistance()\n",
    "eval_function = ChamferDistance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeScale, FixedPoints, Compose\n",
    "from common.transforms.knn_view import KNNSplit\n",
    "\n",
    "pre_transform = NormalizeScale()\n",
    "transform = Compose([FixedPoints(8192), KNNSplit(2048)])\n",
    "\n",
    "root = \"data/ShapeNetAll\"\n",
    "train_dataset = ShapeNet(root=root, categories=None, pre_transform=pre_transform, transform=transform, split=\"train\")\n",
    "val_dataset = ShapeNet(root=root, categories=None, pre_transform=pre_transform, transform=transform, split=\"val\")\n",
    "test_dataset = ShapeNet(root=root, categories=None, pre_transform=pre_transform, transform=transform, split=\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, follow_batch=[\"pos\", \"incomplete\"], drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True, follow_batch=[\"pos\", \"incomplete\"], drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, follow_batch=[\"pos\", \"incomplete\"], drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbone.attdgcnn import AttDGCNNEncoder\n",
    "from models.backbone.pointnet import PointNetEncoder\n",
    "from models.backbone.glu import GLUDecoder\n",
    "\n",
    "\n",
    "class GLUConditionedModel(BaseConditionedModel):\n",
    "    def __init__(self, dim_ctx) -> None:\n",
    "        super().__init__()\n",
    "        self.model = GLUDecoder(dim_ctx=dim_ctx)\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor, ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        return self.model(x=x, t=t, ctx=ctx, batch=batch)\n",
    "\n",
    "\n",
    "encoder = AttDGCNNEncoder(512)\n",
    "model = ConsistencyModel(\n",
    "    model=GLUConditionedModel(512),\n",
    "    resampler=resampler,\n",
    "    parametrization=parametrization,\n",
    ")\n",
    "ema = copy.deepcopy(model)\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()) + list(encoder.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.training import get_data_iterator\n",
    "from common.visualization import visualize_batch_points\n",
    "from common.data import MyData, MyDataBatched\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import socket\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "experiment_name = current_time + \"_\" + socket.gethostname() + \"_dgcnn\"\n",
    "log_dir = os.path.join(\"runs\", experiment_name)\n",
    "ckpt_dir = os.path.join(\"checkpoints\", experiment_name)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "trainer = ConsistencyTrainer(model, ema, step_schedule, time_schedule, ema_decay)\n",
    "encoder = encoder.to(device)\n",
    "model = model.to(device)\n",
    "ema = ema.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch: int, data: MyDataBatched):\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    model.train()\n",
    "    data = data.to(device) # type: ignore\n",
    "\n",
    "    gt, incomplete = data.pos, data.incomplete\n",
    "    gt_batch, incomplete_batch = data.pos_batch, data.incomplete_batch\n",
    "    \n",
    "    incomplete_feat = encoder(incomplete, incomplete_batch)\n",
    "    x_cur, x_ema = trainer.train_step(epoch, gt, incomplete_feat, gt_batch)\n",
    "    \n",
    "    loss = loss_function(x_cur, x_ema, gt_batch)\n",
    "\n",
    "    loss.backward()  # Backward pass.\n",
    "    optimizer.step()  # Update model parameters.\n",
    "    trainer.update_emas(epoch)  # Update EMA parameters.\n",
    "\n",
    "    return loss.item()\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(epoch):\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    group_loss, group_cd = 0.0, 0.0\n",
    "    model.eval()\n",
    "    for data in val_loader:\n",
    "        data: MyDataBatched = data.to(device) # type: ignore\n",
    "\n",
    "        gt, incomplete = data.pos, data.incomplete\n",
    "        gt_batch, incomplete_batch = data.pos_batch, data.incomplete_batch\n",
    "\n",
    "        incomplete_feat = encoder(incomplete, incomplete_batch)\n",
    "        x_cur, x_ema = trainer.train_step(epoch, gt, incomplete_feat, gt_batch)\n",
    "\n",
    "        loss = loss_function(x_cur, x_ema, gt_batch)\n",
    "        eval = eval_function(x_cur, gt, gt_batch)\n",
    "\n",
    "        group_loss, group_cd = group_loss + loss.item(), group_cd + eval.item()\n",
    "\n",
    "    group_loss, group_cd = group_loss / len(val_loader), group_cd / len(val_loader)\n",
    "    return group_loss, group_cd\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(epoch: int):\n",
    "    epoch = epoch + 1\n",
    "    model.eval()\n",
    "\n",
    "    data: MyDataBatched = next(iter(test_loader))\n",
    "    data = data.to(device) # type: ignore\n",
    "    \n",
    "    gt_, incomplete = data.pos, data.incomplete\n",
    "    gt_batch_, incomplete_batch = data.pos_batch, data.incomplete_batch\n",
    "\n",
    "    ones_mask = torch.ones(int(gt_batch_.max() + 1), device=device)\n",
    "    time = settings.max_time * ones_mask\n",
    "    random_base = resampler(gt_, time, gt_batch_)\n",
    "    \n",
    "    incomplete_feat = encoder(incomplete, incomplete_batch)\n",
    "    x_recon = model(random_base, time, incomplete_feat, gt_batch_)\n",
    "\n",
    "    x_fig = visualize_batch_points(incomplete, incomplete_batch)\n",
    "    fig = visualize_batch_points(x_recon, gt_batch_)\n",
    "    return x_fig, fig\n",
    "\n",
    "\n",
    "def training_loop(start=0):\n",
    "    data_iterator = get_data_iterator(train_loader)\n",
    "    failed_last = False\n",
    "    for epoch in range(start, settings.training_iterations + 1):\n",
    "        try:\n",
    "            loss = train(epoch, next(data_iterator))\n",
    "            if epoch % 1 == 0:\n",
    "                print(f\"It: {epoch}, Loss: {loss}\")\n",
    "                writer.add_scalar(\"loss\", loss, epoch)\n",
    "\n",
    "            if epoch and epoch % 1000 == 0:       \n",
    "                x_fig, fig = sample(epoch)\n",
    "                writer.add_figure(\"x_fig\", x_fig, epoch)\n",
    "                writer.add_figure(\"x_recon\", fig, epoch)    \n",
    "                fig.clear()\n",
    "\n",
    "            if epoch and epoch % 1000 == 0:\n",
    "                val_loss, val_cd = validate(epoch)\n",
    "                print(f\"Val It: {epoch}, Loss: {val_loss}, CD: {val_cd}\")\n",
    "                writer.add_scalar(\"val_loss\", val_loss, epoch)\n",
    "                writer.add_scalar(\"val_cd\", val_cd, epoch)\n",
    "\n",
    "\n",
    "            if epoch and epoch % 1000 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'encoder_state_dict': encoder.state_dict(),\n",
    "                    'ema_state_dict': ema.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, os.path.join(ckpt_dir, f\"checkpoint_{epoch}.pth\"))\n",
    "            \n",
    "            failed_last = False\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if failed_last:\n",
    "                break\n",
    "            failed_last = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "It: 0, Loss: 0.2138255089521408\n",
      "torch.Size([8, 512])\n",
      "It: 1, Loss: 0.3212275505065918\n",
      "torch.Size([8, 512])\n",
      "It: 2, Loss: 0.186448872089386\n",
      "torch.Size([8, 512])\n",
      "It: 3, Loss: 0.16066531836986542\n",
      "torch.Size([8, 512])\n",
      "It: 4, Loss: 0.23308484256267548\n",
      "torch.Size([8, 512])\n",
      "It: 5, Loss: 0.21543200314044952\n",
      "torch.Size([8, 512])\n",
      "It: 6, Loss: 0.30318760871887207\n",
      "torch.Size([8, 512])\n",
      "It: 7, Loss: 0.33970481157302856\n",
      "torch.Size([8, 512])\n",
      "It: 8, Loss: 0.3476470708847046\n",
      "torch.Size([8, 512])\n",
      "It: 9, Loss: 0.1481168568134308\n",
      "torch.Size([8, 512])\n",
      "It: 10, Loss: 0.276712030172348\n",
      "torch.Size([8, 512])\n",
      "It: 11, Loss: 0.27033692598342896\n",
      "torch.Size([8, 512])\n",
      "It: 12, Loss: 0.10079048573970795\n",
      "torch.Size([8, 512])\n",
      "It: 13, Loss: 0.15949709713459015\n",
      "torch.Size([8, 512])\n",
      "It: 14, Loss: 0.4224643409252167\n",
      "torch.Size([8, 512])\n",
      "It: 15, Loss: 0.13747812807559967\n",
      "torch.Size([8, 512])\n",
      "It: 16, Loss: 0.27220574021339417\n",
      "torch.Size([8, 512])\n",
      "It: 17, Loss: 0.25022849440574646\n",
      "torch.Size([8, 512])\n",
      "It: 18, Loss: 0.42209064960479736\n",
      "torch.Size([8, 512])\n",
      "It: 19, Loss: 0.21539950370788574\n",
      "torch.Size([8, 512])\n",
      "It: 20, Loss: 0.20971551537513733\n",
      "torch.Size([8, 512])\n",
      "It: 21, Loss: 0.24227654933929443\n",
      "torch.Size([8, 512])\n",
      "It: 22, Loss: 0.16301050782203674\n",
      "torch.Size([8, 512])\n",
      "It: 23, Loss: 0.02547074668109417\n",
      "torch.Size([8, 512])\n",
      "It: 24, Loss: 0.16215907037258148\n",
      "torch.Size([8, 512])\n",
      "It: 25, Loss: 0.27123913168907166\n",
      "torch.Size([8, 512])\n",
      "It: 26, Loss: 0.1914289891719818\n",
      "torch.Size([8, 512])\n",
      "It: 27, Loss: 0.2635518014431\n",
      "torch.Size([8, 512])\n",
      "It: 28, Loss: 0.15665680170059204\n",
      "torch.Size([8, 512])\n",
      "It: 29, Loss: 0.24306820333003998\n",
      "torch.Size([8, 512])\n",
      "It: 30, Loss: 0.07830530405044556\n",
      "torch.Size([8, 512])\n",
      "It: 31, Loss: 0.24549265205860138\n",
      "torch.Size([8, 512])\n",
      "It: 32, Loss: 0.2294471710920334\n",
      "torch.Size([8, 512])\n",
      "It: 33, Loss: 3.737763108802028e-05\n",
      "torch.Size([8, 512])\n",
      "It: 34, Loss: 0.08484035730361938\n",
      "torch.Size([8, 512])\n",
      "It: 35, Loss: 0.10017725825309753\n",
      "torch.Size([8, 512])\n",
      "It: 36, Loss: 0.1463531106710434\n",
      "torch.Size([8, 512])\n",
      "It: 37, Loss: 0.21772412955760956\n",
      "torch.Size([8, 512])\n",
      "It: 38, Loss: 0.3348153829574585\n",
      "torch.Size([8, 512])\n",
      "It: 39, Loss: 0.04806354269385338\n",
      "torch.Size([8, 512])\n",
      "It: 40, Loss: 0.1976107805967331\n",
      "torch.Size([8, 512])\n",
      "It: 41, Loss: 0.10239766538143158\n",
      "torch.Size([8, 512])\n",
      "It: 42, Loss: 0.16760411858558655\n",
      "torch.Size([8, 512])\n",
      "It: 43, Loss: 0.08034201711416245\n",
      "torch.Size([8, 512])\n",
      "It: 44, Loss: 0.1284818947315216\n",
      "torch.Size([8, 512])\n",
      "It: 45, Loss: 0.19481560587882996\n",
      "torch.Size([8, 512])\n",
      "It: 46, Loss: 2.250591387564782e-05\n",
      "torch.Size([8, 512])\n",
      "It: 47, Loss: 0.15963071584701538\n",
      "torch.Size([8, 512])\n",
      "It: 48, Loss: 0.1437087506055832\n",
      "torch.Size([8, 512])\n",
      "It: 49, Loss: 0.09896507114171982\n",
      "torch.Size([8, 512])\n",
      "It: 50, Loss: 0.09605808556079865\n",
      "torch.Size([8, 512])\n",
      "It: 51, Loss: 0.08813726902008057\n",
      "torch.Size([8, 512])\n",
      "It: 52, Loss: 0.1150326132774353\n",
      "torch.Size([8, 512])\n",
      "It: 53, Loss: 0.0011369975982233882\n",
      "torch.Size([8, 512])\n",
      "It: 54, Loss: 0.17778298258781433\n",
      "torch.Size([8, 512])\n",
      "It: 55, Loss: 0.024653716012835503\n",
      "torch.Size([8, 512])\n",
      "It: 56, Loss: 0.05517023801803589\n",
      "torch.Size([8, 512])\n",
      "It: 57, Loss: 0.08702728152275085\n",
      "torch.Size([8, 512])\n",
      "It: 58, Loss: 0.13705003261566162\n",
      "torch.Size([8, 512])\n",
      "It: 59, Loss: 0.09289683401584625\n",
      "torch.Size([8, 512])\n",
      "It: 60, Loss: 0.00044333806727081537\n",
      "torch.Size([8, 512])\n",
      "It: 61, Loss: 0.055304091423749924\n",
      "torch.Size([8, 512])\n",
      "It: 62, Loss: 0.11330410093069077\n",
      "torch.Size([8, 512])\n",
      "It: 63, Loss: 0.10429702699184418\n",
      "torch.Size([8, 512])\n",
      "It: 64, Loss: 0.11819394677877426\n",
      "torch.Size([8, 512])\n",
      "It: 65, Loss: 0.21881236135959625\n",
      "torch.Size([8, 512])\n",
      "It: 66, Loss: 0.0684242993593216\n",
      "torch.Size([8, 512])\n",
      "It: 67, Loss: 0.0006055569392628968\n",
      "torch.Size([8, 512])\n",
      "It: 68, Loss: 0.13741280138492584\n",
      "torch.Size([8, 512])\n",
      "It: 69, Loss: 0.0007101010414771736\n",
      "torch.Size([8, 512])\n",
      "It: 70, Loss: 0.09765443205833435\n",
      "torch.Size([8, 512])\n",
      "It: 71, Loss: 0.08528454601764679\n",
      "torch.Size([8, 512])\n",
      "It: 72, Loss: 0.0533907413482666\n",
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "state_file = None\n",
    "if state_file is not None:\n",
    "    state = torch.load(state_file)\n",
    "    start = state['epoch'] + 1\n",
    "    encoder.load_state_dict(state['encoder_state_dict'])\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    ema.load_state_dict(state['ema_state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print(f\"Loaded checkpoint from {state_file}\")\n",
    "else:\n",
    "    start = 0\n",
    "    ema.load_state_dict(model.state_dict()) \n",
    "\n",
    "training_loop(start=start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
