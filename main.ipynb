{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as gnn\n",
    "from torch import Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConsistencySettings:\n",
    "    training_iterations: int = 100_000\n",
    "    min_time_partitions: int = 1\n",
    "    max_time_partitions: int = 150\n",
    "    initial_ema_decay: float = 0.99\n",
    "    min_time: float = 1e-4\n",
    "    data_time: float = 1e-3\n",
    "    max_time: float = 1.0\n",
    "    rho: float = 7.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from loss.chamfer_loss import CDLoss\n",
    "from loss.emd_loss import EMDLoss\n",
    "\n",
    "class BaseDistanceFunc(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def __call__(self, lhs: Tensor, rhs: Tensor, batch: Tensor | None) -> Tensor:\n",
    "        ...\n",
    "    \n",
    "\n",
    "class ChamferDistance(BaseDistanceFunc):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.loss = CDLoss()\n",
    "        \n",
    "\n",
    "    def __call__(self, lhs: Tensor, rhs: Tensor, batch: Tensor | None) -> Tensor:\n",
    "        return self.loss(lhs, rhs, batch)\n",
    "    \n",
    "\n",
    "class MSEDistance(BaseDistanceFunc):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def __call__(self, lhs: Tensor, rhs: Tensor, batch: Tensor | None) -> Tensor:\n",
    "        return self.loss(lhs, rhs)\n",
    "\n",
    "\n",
    "class EarthMoverDistance(BaseDistanceFunc):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.loss = EMDLoss()\n",
    "\n",
    "    def __call__(self, lhs: Tensor, rhs: Tensor, batch: Tensor | None) -> Tensor:\n",
    "        return self.loss(lhs, rhs, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class BaseNumTimestepsSchedule(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, iteration: int) -> int:\n",
    "        ...\n",
    "    \n",
    "\n",
    "class NumTimestepsSchedule(BaseNumTimestepsSchedule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        min_time_partitions: int,\n",
    "        max_time_partitions: int,\n",
    "        training_iterations: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target_disc_steps = (min_time_partitions, max_time_partitions)\n",
    "        self.training_iterations = training_iterations\n",
    "\n",
    "    def __call__(self, iteration: int) -> int:\n",
    "        s, K = self.target_disc_steps, self.training_iterations\n",
    "        num_timesteps = (s[1] + 1)**2 - s[0]**2\n",
    "        num_timesteps = iteration * num_timesteps / K\n",
    "        num_timesteps = num_timesteps + s[0]**2\n",
    "        num_timesteps = math.sqrt(num_timesteps)\n",
    "        num_timesteps = math.ceil(-1. + num_timesteps)\n",
    "        return 1 + num_timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseTimeSchedule(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, up_to: int, device: torch.device | None=None) -> torch.Tensor:\n",
    "        ...\n",
    "    \n",
    "    \n",
    "class KarrasTimeSchedule(BaseTimeSchedule):\n",
    "    def __init__(self, min_time: float, max_time: float, rho: float) -> None:\n",
    "        super().__init__()\n",
    "        self.sigma_range = (min_time, max_time)\n",
    "        self.rho = rho\n",
    "\n",
    "    def __call__(self, up_to: int, device: torch.device | None=None) -> torch.Tensor:\n",
    "        (eps, T), rho = self.sigma_range, self.rho\n",
    "        rho_inv = 1.0 / rho\n",
    "        steps = torch.arange(up_to, device=device) / max(up_to - 1, 1)\n",
    "        sigmas = eps**rho_inv + steps * (T**rho_inv - eps**rho_inv)\n",
    "        sigmas = sigmas**rho\n",
    "        return sigmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseEMADecay(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, iteration: int) -> float:\n",
    "        ...\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "class ExponentialDecay(BaseEMADecay):\n",
    "    def __init__(self, initial_decay: float, training_iterations: int) -> None:\n",
    "        super().__init__()\n",
    "        self.initial_decay = initial_decay\n",
    "        self.training_iterations = training_iterations\n",
    "\n",
    "    def __call__(self, iteration: int) -> float:\n",
    "        return math.exp(\n",
    "            iteration * math.log(self.initial_decay) / self.training_iterations\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseParametrization(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: Tensor, y: Tensor, t: Tensor, batch: Tensor) -> Tensor:\n",
    "        ...\n",
    "        \n",
    "\n",
    "class BaseResampler(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: Tensor, t: Tensor, batch: Tensor) -> Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class EpsilonParametrization(BaseParametrization):\n",
    "    def __init__(self, min_time: float, data_time: float) -> None:\n",
    "        super().__init__()\n",
    "        self.data_time = data_time\n",
    "        self.min_time = min_time\n",
    "\n",
    "    def skip(self, t: Tensor) -> Tensor:\n",
    "        return (self.data_time ** 2) / ((t - self.min_time) ** 2 + (self.data_time ** 2))\n",
    "\n",
    "    def out(self, t: Tensor) -> Tensor:\n",
    "        return (t - self.min_time) * self.data_time / (self.data_time**2 + t**2) ** 0.5    \n",
    "\n",
    "    def __call__(self, x: Tensor, y: Tensor, t: Tensor, batch: Tensor) -> Tensor:\n",
    "        return self.skip(t)[batch, None] * x + self.out(t)[batch, None] * y \n",
    "    \n",
    "\n",
    "class EpsilonResampler(BaseResampler):\n",
    "    def __init__(self, min_time: float) -> None:\n",
    "        super().__init__()\n",
    "        self.min_time = min_time\n",
    "\n",
    "    def __call__(self, x: Tensor, t: Tensor, batch: Tensor) -> Tensor:\n",
    "        mul = (t**2 - self.min_time**2)**0.5\n",
    "        return x + mul[batch, None] * torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class BaseConditionedModel(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor, t: Tensor, ctx: torch.Tensor, batch: Tensor) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x: Tensor, t: Tensor, ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        return super().__call__(x=x, t=t, ctx=ctx, batch=batch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyModel(nn.Module):\n",
    "    def __init__(self, model: BaseConditionedModel, resampler: BaseResampler, parametrization: BaseParametrization) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.resampler = resampler\n",
    "        self.parametrization = parametrization\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor | tuple[Tensor], ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        assert not isinstance(t, int)\n",
    "        ts = (t,) if isinstance(t, Tensor) else t\n",
    "\n",
    "        x = self.wrapped_model(x=x, t=ts[0], ctx=ctx, batch=batch)\n",
    "        for t in ts[1:]:\n",
    "            x = self.resampler(x=x, t=t, batch=batch)\n",
    "            x = self.wrapped_model(x=x, t=t, ctx=ctx, batch=batch)\n",
    "        return x\n",
    "\n",
    "    def wrapped_model(self, x: Tensor, t: Tensor, ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        y = self.model(x=x, t=t, ctx=ctx, batch=batch)\n",
    "        return self.parametrization(x=x, y=y, t=t, batch=batch)\n",
    "\n",
    "    def __call__(self, x: Tensor, t: Tensor | tuple[Tensor], ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        return super().__call__(x=x, t=t, ctx=ctx, batch=batch)\n",
    "\n",
    "\n",
    "class ConsistencyTrainer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 model: ConsistencyModel, \n",
    "                 ema: ConsistencyModel,\n",
    "                 step_schedule: BaseNumTimestepsSchedule, \n",
    "                 time_schedule: BaseTimeSchedule,\n",
    "                 ema_decay: BaseEMADecay):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.ema = ema\n",
    "\n",
    "        self.step_schedule = step_schedule\n",
    "        self.time_schedule = time_schedule\n",
    "        self.ema_decay = ema_decay\n",
    "        \n",
    "    def train_step(self, iteration: int, x: Tensor, ctx: Tensor, batch: Tensor):\n",
    "        num_timesteps = self.step_schedule(iteration)\n",
    "        times = self.time_schedule(num_timesteps, device=x.device)\n",
    "        \n",
    "        batch_size = int(batch.max()) + 1\n",
    "        time_indices = torch.randint(0, num_timesteps - 1, (batch_size,), device=x.device)\n",
    "        current_times = times[time_indices]\n",
    "        next_times = times[time_indices + 1]\n",
    "\n",
    "        z = torch.randn_like(x, device=x.device)\n",
    "        next_x = x + z * next_times[batch, None]\n",
    "        denoised = self.model(\n",
    "            x=next_x, \n",
    "            t=next_times, \n",
    "            ctx=ctx,\n",
    "            batch=batch, \n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            current_x = x + z * current_times[batch, None]\n",
    "            ema_denoised = self.ema(\n",
    "                x=current_x, \n",
    "                t=current_times, \n",
    "                ctx=ctx,\n",
    "                batch=batch, \n",
    "            )\n",
    "\n",
    "        return denoised, ema_denoised \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_emas(self, iteration: int):\n",
    "        alpha = self.ema_decay(iteration)\n",
    "        for p, ema_p in zip(self.model.parameters(), self.ema.parameters()):\n",
    "            ema_p.data = alpha * ema_p.data + (1 - alpha) * p.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbone.pointnet import PointNetEncoder\n",
    "from models.backbone.glu import GLUDecoder\n",
    "\n",
    "\n",
    "class GLUConditionedModel(BaseConditionedModel):\n",
    "    def __init__(self, dim_ctx) -> None:\n",
    "        super().__init__()\n",
    "        self.model = GLUDecoder(dim_ctx=dim_ctx)\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor, ctx: Tensor, batch: Tensor) -> Tensor:\n",
    "        return self.model(x=x, t=t, ctx=ctx, batch=batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeScale, FixedPoints, Compose, NormalizeRotation\n",
    "\n",
    "pre_transform = NormalizeScale()\n",
    "transform = FixedPoints(1024)\n",
    "\n",
    "root = \"data/ShapeNet\"\n",
    "train_dataset = ShapeNet(root=root, categories=[\"Airplane\"], pre_transform=pre_transform, transform=transform, split=\"train\")\n",
    "val_dataset = ShapeNet(root=root, categories=[\"Airplane\"], pre_transform=pre_transform, transform=transform, split=\"val\")\n",
    "test_dataset = ShapeNet(root=root, categories=[\"Airplane\"], pre_transform=pre_transform, transform=transform, split=\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = ConsistencySettings(\n",
    "    training_iterations=100_000,\n",
    "    min_time_partitions=2,\n",
    "    max_time_partitions=150,\n",
    "    initial_ema_decay=0.95,\n",
    "    min_time=1e-4,\n",
    "    data_time=1e-3,\n",
    "    max_time=1.0,\n",
    "    rho=7.0\n",
    ")\n",
    "\n",
    "step_schedule = NumTimestepsSchedule(\n",
    "    settings.min_time_partitions,\n",
    "    settings.max_time_partitions,\n",
    "    settings.training_iterations\n",
    ")\n",
    "time_schedule = KarrasTimeSchedule(\n",
    "    settings.min_time,\n",
    "    settings.max_time,\n",
    "    settings.rho\n",
    ")\n",
    "ema_decay = ExponentialDecay(\n",
    "    settings.initial_ema_decay,\n",
    "    settings.training_iterations\n",
    ")\n",
    "parametrization = EpsilonParametrization(\n",
    "    settings.min_time,\n",
    "    settings.data_time\n",
    ")\n",
    "resampler = EpsilonResampler(\n",
    "    settings.min_time\n",
    ")\n",
    "loss_function = ChamferDistance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.training import get_data_iterator\n",
    "from common.visualization import visualize_batch_results\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import socket\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "experiment_name = current_time + \"_\" + socket.gethostname() + \"_consistency\"\n",
    "log_dir = os.path.join(\"runs\", experiment_name)\n",
    "ckpt_dir = os.path.join(\"checkpoints\", experiment_name)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = PointNetEncoder(256).to(device)\n",
    "model = ConsistencyModel(\n",
    "    model=GLUConditionedModel(256),\n",
    "    resampler=resampler,\n",
    "    parametrization=parametrization\n",
    ").to(device)\n",
    "ema = copy.deepcopy(model)\n",
    "ema.load_state_dict(model.state_dict())\n",
    "ema.eval()\n",
    "ema = ema.to(device)\n",
    "\n",
    "\n",
    "trainer = ConsistencyTrainer(model, ema, step_schedule, time_schedule, ema_decay)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(encoder.parameters()), lr=1e-3)\n",
    "\n",
    "# state_file = 'checkpoints/Oct29_22-53-03_fedora_consistency/checkpoint_13000.pth'\n",
    "# if state_file is not None:\n",
    "#     state = torch.load(state_file)\n",
    "#     model.load_state_dict(state['model_state_dict'])\n",
    "#     encoder.load_state_dict(state['encoder_state_dict'])\n",
    "#     ema.load_state_dict(state['ema_state_dict'])\n",
    "#     optimizer.load_state_dict(state['optimizer'])\n",
    "#     start_epoch = state['epoch']\n",
    "#     print(f\"Loaded checkpoint from {state_file}\")\n",
    "\n",
    "\n",
    "def train(epoch, data):\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    model.train()\n",
    "    \n",
    "    data = data.to(device)\n",
    "    x: Tensor = data.pos\n",
    "    batch: Tensor = data.batch\n",
    "\n",
    "    ctx = encoder(x, batch)\n",
    "    x_cur, x_ema = trainer.train_step(epoch, x, ctx, batch)\n",
    "    loss = loss_function(x_cur, x_ema, batch)\n",
    "\n",
    "    loss.backward()  # Backward pass.\n",
    "    optimizer.step()  # Update model parameters.\n",
    "    trainer.update_emas(epoch)\n",
    "\n",
    "    return loss.item()\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(epoch):\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    group_loss = 0.0\n",
    "    for data in val_loader:\n",
    "        model.eval()\n",
    "        data = data.to(device)\n",
    "        x: Tensor = data.pos\n",
    "        batch: Tensor = data.batch\n",
    "\n",
    "        ctx = encoder(x, batch)\n",
    "        x_cur, x_ema = trainer.train_step(epoch, x, ctx, batch)\n",
    "        loss = loss_function(x_cur, x_ema, batch)\n",
    "        group_loss += loss.item()\n",
    "\n",
    "    group_loss /= len(val_loader)\n",
    "    return group_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(epoch):\n",
    "    epoch = epoch + 1\n",
    "    model.eval()\n",
    "\n",
    "    data = next(iter(test_loader))\n",
    "    data = data.to(device)\n",
    "    x: Tensor = data.pos\n",
    "    batch: Tensor = data.batch\n",
    "\n",
    "    ctx = encoder(x, batch)\n",
    "    x_cur, x_ema = trainer.train_step(epoch, x, ctx, batch)\n",
    "\n",
    "    fig = visualize_batch_results(x_cur, batch)\n",
    "    fig_ema = visualize_batch_results(x_ema, batch)\n",
    "    return fig, fig_ema\n",
    "\n",
    "\n",
    "def training_loop():\n",
    "    data_iterator = get_data_iterator(train_loader)\n",
    "    for epoch in range(0, settings.training_iterations):\n",
    "        loss = train(epoch, next(data_iterator))\n",
    "\n",
    "        print(f\"It: {epoch}, Loss: {loss}\")\n",
    "        if epoch % 100 == 0:\n",
    "            writer.add_scalar(\"loss\", loss, epoch)\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            val_loss = validate(epoch)\n",
    "            fig, fig_ema = sample(epoch)\n",
    "\n",
    "            print(f\"Val It: {epoch}, Loss: {val_loss}\")\n",
    "            writer.add_scalar(\"val_loss\", val_loss, epoch)\n",
    "            writer.add_figure(\"test_fig\", fig, epoch)    \n",
    "            writer.add_figure(\"test_fig_ema\", fig_ema, epoch)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'ema_state_dict': ema.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, os.path.join(ckpt_dir, f\"checkpoint_{epoch}.pth\"))\n",
    "\n",
    "training_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
