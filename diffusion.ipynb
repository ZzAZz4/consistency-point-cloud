{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, SiLU, ModuleList\n",
    "from torch_geometric.nn import MessagePassing, MLP, AttentionalAggregation, MaxAggregation\n",
    "from torch_geometric.nn import PointNetConv, PositionalEncoding\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from torch_cluster import knn_graph, fps\n",
    "\n",
    "\n",
    "class PointNetEncoder(torch.nn.Module):\n",
    "    def __init__(self, zdim):\n",
    "        super().__init__()\n",
    "        self.conv1 = PointNetConv(\n",
    "            local_nn=MLP([3 + 3, 32], act=SiLU(), plain_last=True), \n",
    "            global_nn=SiLU(), \n",
    "            aggr=MaxAggregation()\n",
    "        )\n",
    "        self.conv2 = PointNetConv(\n",
    "            local_nn=MLP([32 + 3, 32], act=SiLU(), plain_last=True), \n",
    "            global_nn=SiLU(),\n",
    "            aggr=MaxAggregation()\n",
    "        )\n",
    "        self.aggr = MaxAggregation()\n",
    "        self.net = Linear(32, zdim)\n",
    "\n",
    "    def forward(self, pos: torch.Tensor, batch: torch.Tensor):\n",
    "        h: torch.Tensor\n",
    "        edge_index = knn_graph(pos, k=16, batch=batch, loop=True)\n",
    "        h = self.conv1(x=pos, pos=pos, edge_index=edge_index)\n",
    "\n",
    "        index = fps(pos, batch, ratio=0.5)\n",
    "        h, pos, batch = h[index], pos[index], batch[index]\n",
    "        edge_index = knn_graph(pos, k=16, batch=batch, loop=True)\n",
    "        h = self.conv2(x=h, pos=pos, edge_index=edge_index)\n",
    "\n",
    "        h = self.aggr(h, batch)  # [batch_size, hidden_channels]\n",
    "        return self.net(h)\n",
    "    \n",
    "\n",
    "class ConcatSquashLinear(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_ctx):\n",
    "        super(ConcatSquashLinear, self).__init__()\n",
    "        self._layer = Linear(dim_in, dim_out)\n",
    "        self._hyper_bias = Linear(dim_ctx, dim_out, bias=False)\n",
    "        self._hyper_gate = Linear(dim_ctx, dim_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, ctx: torch.Tensor, batch: torch.Tensor):\n",
    "        gate: torch.Tensor = torch.sigmoid(self._hyper_gate(ctx))\n",
    "        bias: torch.Tensor = self._hyper_bias(ctx)\n",
    "        ret: torch.Tensor = self._layer(x) * gate[batch] + bias[batch]\n",
    "        return ret\n",
    "\n",
    "\n",
    "class PointwiseNet(torch.nn.Module):\n",
    "    def __init__(self, dim_ctx):\n",
    "        super().__init__()\n",
    "        self.net = ModuleList([\n",
    "            ConcatSquashLinear(3, 128, dim_ctx),\n",
    "            ConcatSquashLinear(128, 256, dim_ctx),\n",
    "            ConcatSquashLinear(256, 512, dim_ctx),\n",
    "            ConcatSquashLinear(512, 256, dim_ctx),\n",
    "            ConcatSquashLinear(256, 128, dim_ctx),\n",
    "        ])\n",
    "        self.out = ConcatSquashLinear(128, 3, dim_ctx)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, ctx: torch.Tensor, batch: torch.Tensor):\n",
    "        out: torch.Tensor = x\n",
    "        for layer in self.net:\n",
    "            out = layer(out, ctx, batch)\n",
    "            out = torch.nn.functional.silu(out)\n",
    "\n",
    "        out = self.out(out, ctx, batch)\n",
    "        return x + out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeometricShapes(40)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import GeometricShapes\n",
    "from torch_geometric.transforms import NormalizeScale, SamplePoints, Compose\n",
    "\n",
    "transform = Compose([NormalizeScale(), SamplePoints(1024)])\n",
    "dataset = GeometricShapes(root='data/GeometricShapes', transform=transform)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 3.8056, Test Accuracy: 0.0250\n",
      "Epoch: 02, Loss: 3.6662, Test Accuracy: 0.0500\n",
      "Epoch: 03, Loss: 3.6004, Test Accuracy: 0.0250\n",
      "Epoch: 04, Loss: 3.5283, Test Accuracy: 0.0500\n",
      "Epoch: 05, Loss: 3.4957, Test Accuracy: 0.0500\n",
      "Epoch: 06, Loss: 3.4506, Test Accuracy: 0.0500\n",
      "Epoch: 07, Loss: 3.4021, Test Accuracy: 0.0750\n",
      "Epoch: 08, Loss: 3.3420, Test Accuracy: 0.1000\n",
      "Epoch: 09, Loss: 3.2696, Test Accuracy: 0.1750\n",
      "Epoch: 10, Loss: 3.1986, Test Accuracy: 0.1750\n",
      "Epoch: 11, Loss: 3.0949, Test Accuracy: 0.1750\n",
      "Epoch: 12, Loss: 2.9843, Test Accuracy: 0.2000\n",
      "Epoch: 13, Loss: 2.8761, Test Accuracy: 0.2250\n",
      "Epoch: 14, Loss: 2.7336, Test Accuracy: 0.2500\n",
      "Epoch: 15, Loss: 2.6267, Test Accuracy: 0.3000\n",
      "Epoch: 16, Loss: 2.4353, Test Accuracy: 0.3750\n",
      "Epoch: 17, Loss: 2.3108, Test Accuracy: 0.4250\n",
      "Epoch: 18, Loss: 2.1443, Test Accuracy: 0.5250\n",
      "Epoch: 19, Loss: 1.9847, Test Accuracy: 0.6750\n",
      "Epoch: 20, Loss: 1.8719, Test Accuracy: 0.7500\n",
      "Epoch: 21, Loss: 1.7290, Test Accuracy: 0.6750\n",
      "Epoch: 22, Loss: 1.5670, Test Accuracy: 0.7000\n",
      "Epoch: 23, Loss: 1.4206, Test Accuracy: 0.7250\n",
      "Epoch: 24, Loss: 1.4212, Test Accuracy: 0.7500\n",
      "Epoch: 25, Loss: 1.2216, Test Accuracy: 0.8250\n",
      "Epoch: 26, Loss: 1.1506, Test Accuracy: 0.8000\n",
      "Epoch: 27, Loss: 1.1689, Test Accuracy: 0.7750\n",
      "Epoch: 28, Loss: 0.9084, Test Accuracy: 0.8000\n",
      "Epoch: 29, Loss: 0.9639, Test Accuracy: 0.8000\n",
      "Epoch: 30, Loss: 0.7731, Test Accuracy: 0.8250\n",
      "Epoch: 31, Loss: 0.8856, Test Accuracy: 0.8000\n",
      "Epoch: 32, Loss: 0.8931, Test Accuracy: 0.8250\n",
      "Epoch: 33, Loss: 0.7901, Test Accuracy: 0.8750\n",
      "Epoch: 34, Loss: 0.7511, Test Accuracy: 0.8000\n",
      "Epoch: 35, Loss: 0.7535, Test Accuracy: 0.7750\n",
      "Epoch: 36, Loss: 0.6526, Test Accuracy: 0.8500\n",
      "Epoch: 37, Loss: 0.6566, Test Accuracy: 0.8250\n",
      "Epoch: 38, Loss: 0.6502, Test Accuracy: 0.8750\n",
      "Epoch: 39, Loss: 0.5873, Test Accuracy: 0.8500\n",
      "Epoch: 40, Loss: 0.6245, Test Accuracy: 0.8250\n",
      "Epoch: 41, Loss: 0.6440, Test Accuracy: 0.8250\n",
      "Epoch: 42, Loss: 0.5152, Test Accuracy: 0.8500\n",
      "Epoch: 43, Loss: 0.5022, Test Accuracy: 0.8500\n",
      "Epoch: 44, Loss: 0.5480, Test Accuracy: 0.9250\n",
      "Epoch: 45, Loss: 0.4915, Test Accuracy: 0.9000\n",
      "Epoch: 46, Loss: 0.4463, Test Accuracy: 0.8250\n",
      "Epoch: 47, Loss: 0.4687, Test Accuracy: 0.8750\n",
      "Epoch: 48, Loss: 0.4375, Test Accuracy: 0.9250\n",
      "Epoch: 49, Loss: 0.5392, Test Accuracy: 0.8250\n",
      "Epoch: 50, Loss: 0.5404, Test Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_dataset = GeometricShapes(root='data/GeometricShapes', train=True,\n",
    "                                transform=SamplePoints(128))\n",
    "test_dataset = GeometricShapes(root='data/GeometricShapes', train=False,\n",
    "                               transform=SamplePoints(128))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10)\n",
    "\n",
    "model = PointNetEncoder(zdim=train_dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train() \n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        logits = model(data.pos, data.batch)  # Forward pass.\n",
    "        loss = criterion(logits, data.y)  # Loss computation.\n",
    "        loss.backward()  # Backward pass.\n",
    "        optimizer.step()  # Update model parameters.\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset) # type: ignore\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    for data in loader:\n",
    "        logits = model(data.pos, data.batch)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += int((pred == data.y).sum())\n",
    "\n",
    "    return total_correct / len(loader.dataset)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss = train(model, optimizer, train_loader)\n",
    "    test_acc = test(model, test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class VarianceSchedule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_steps, beta_1, beta_T):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_T = beta_T\n",
    "\n",
    "        betas = torch.linspace(beta_1, beta_T, steps=num_steps)\n",
    "        betas = torch.cat([torch.zeros([1]), betas], dim=0)     # Padding\n",
    "\n",
    "        alphas = 1 - betas\n",
    "        log_alphas = torch.log(alphas)\n",
    "        for i in range(1, log_alphas.size(0)):  # 1 to T\n",
    "            log_alphas[i] += log_alphas[i - 1]\n",
    "        alpha_bars = log_alphas.exp()\n",
    "\n",
    "        sigmas_flex = torch.sqrt(betas)\n",
    "        sigmas_inflex = torch.zeros_like(sigmas_flex)\n",
    "        for i in range(1, sigmas_flex.size(0)):\n",
    "            sigmas_inflex[i] = ((1 - alpha_bars[i-1]) / (1 - alpha_bars[i])) * betas[i]\n",
    "        sigmas_inflex = torch.sqrt(sigmas_inflex)\n",
    "\n",
    "        self.betas: torch.Tensor\n",
    "        self.alphas: torch.Tensor\n",
    "        self.alpha_bars: torch.Tensor\n",
    "        self.sigmas_flex: torch.Tensor\n",
    "        self.sigmas_inflex: torch.Tensor\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alpha_bars', alpha_bars)\n",
    "        self.register_buffer('sigmas_flex', sigmas_flex)\n",
    "        self.register_buffer('sigmas_inflex', sigmas_inflex)\n",
    "\n",
    "    def uniform_sample_t(self, batch_size):\n",
    "        ts = np.random.choice(np.arange(1, self.num_steps+1), batch_size)\n",
    "        return ts.tolist()\n",
    "\n",
    "    def get_sigmas(self, t, flexibility):\n",
    "        assert 0 <= flexibility and flexibility <= 1\n",
    "        sigmas = self.sigmas_flex[t] * flexibility + self.sigmas_inflex[t] * (1 - flexibility)\n",
    "        return sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, zdim, num_steps, beta_1, beta_T):\n",
    "        super().__init__()\n",
    "        self.encoder = PointNetEncoder(zdim)\n",
    "        self.decoder = PointwiseNet(zdim)\n",
    "        self.schedule = VarianceSchedule(num_steps, beta_1, beta_T)\n",
    "\n",
    "    def forward(self, pos: torch.Tensor, batch: torch.Tensor):\n",
    "        z: torch.Tensor = self.encoder(pos, batch)\n",
    "        batch_size = z.size(0)\n",
    " \n",
    "        t = 43 * torch.ones(batch_size, dtype=torch.long)\n",
    "        alpha_bar = self.schedule.alpha_bars[t]\n",
    "        # beta = self.schedule.betas[t]\n",
    "\n",
    "        c0 = torch.sqrt(alpha_bar)       # (B, 1, 1)\n",
    "        c1 = torch.sqrt(1 - alpha_bar)   # (B, 1, 1)\n",
    "        c0, c1 = c0[batch].view(-1, 1), c1[batch].view(-1, 1)\n",
    "\n",
    "        e_rand = torch.randn_like(pos)\n",
    "        e_theta = self.decoder(c0 * pos + c1 * e_rand, ctx=z, batch=batch)\n",
    "\n",
    "        return e_theta, e_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_iterator(iterable):\n",
    "    \"\"\"Allows training with DataLoaders in a single infinite loop:\n",
    "        for i, data in enumerate(inf_generator(train_loader)):\n",
    "    \"\"\"\n",
    "    iterator = iterable.__iter__()\n",
    "    while True:\n",
    "        try:\n",
    "            yield iterator.__next__()\n",
    "        except StopIteration:\n",
    "            iterator = iterable.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Loss: 1.6197115182876587\n",
      "Iteration: 2, Loss: 0.8131834864616394\n",
      "Iteration: 3, Loss: 0.6480311155319214\n",
      "Iteration: 4, Loss: 0.6618724465370178\n",
      "Iteration: 5, Loss: 0.6184800863265991\n",
      "Iteration: 6, Loss: 1.0993030071258545\n",
      "Iteration: 7, Loss: 0.6762861609458923\n",
      "Iteration: 8, Loss: 0.9229393005371094\n",
      "Iteration: 9, Loss: 0.6136332750320435\n",
      "Iteration: 10, Loss: 0.7686660289764404\n",
      "Iteration: 11, Loss: 4.202695846557617\n",
      "Iteration: 12, Loss: 0.7083160877227783\n",
      "Iteration: 13, Loss: 0.7865323424339294\n",
      "Iteration: 14, Loss: 1.1750801801681519\n",
      "Iteration: 15, Loss: 0.8006055951118469\n",
      "Iteration: 16, Loss: 0.624251127243042\n",
      "Iteration: 17, Loss: 0.6602266430854797\n",
      "Iteration: 18, Loss: 0.9424404501914978\n",
      "Iteration: 19, Loss: 1.1071655750274658\n",
      "Iteration: 20, Loss: 0.717014491558075\n",
      "Iteration: 21, Loss: 0.6872360110282898\n",
      "Iteration: 22, Loss: 0.6284244656562805\n",
      "Iteration: 23, Loss: 0.7244883179664612\n",
      "Iteration: 24, Loss: 1.4483073949813843\n",
      "Iteration: 25, Loss: 1.0629996061325073\n",
      "Iteration: 26, Loss: 0.8697062730789185\n",
      "Iteration: 27, Loss: 0.7379348278045654\n",
      "Iteration: 28, Loss: 0.620488703250885\n",
      "Iteration: 29, Loss: 0.6706072688102722\n",
      "Iteration: 30, Loss: 2.484477996826172\n",
      "Iteration: 31, Loss: 1.1561627388000488\n",
      "Iteration: 32, Loss: 0.654780387878418\n",
      "Iteration: 33, Loss: 0.7196245789527893\n",
      "Iteration: 34, Loss: 0.5777785181999207\n",
      "Iteration: 35, Loss: 1.3545660972595215\n",
      "Iteration: 36, Loss: 0.8119286298751831\n",
      "Iteration: 37, Loss: 0.6535965800285339\n",
      "Iteration: 38, Loss: 1.0289766788482666\n",
      "Iteration: 39, Loss: 0.6533825993537903\n",
      "Iteration: 40, Loss: 1.1036003828048706\n",
      "Iteration: 41, Loss: 0.6507474184036255\n",
      "Iteration: 42, Loss: 0.7287804484367371\n",
      "Iteration: 43, Loss: 1.1412206888198853\n",
      "Iteration: 44, Loss: 0.7641197443008423\n",
      "Iteration: 45, Loss: 0.6243085861206055\n",
      "Iteration: 46, Loss: 0.928672730922699\n",
      "Iteration: 47, Loss: 0.9203030467033386\n",
      "Iteration: 48, Loss: 0.7385259866714478\n",
      "Iteration: 49, Loss: 0.689814031124115\n",
      "Iteration: 50, Loss: 0.8169939517974854\n",
      "Iteration: 51, Loss: 0.8180823922157288\n",
      "Iteration: 52, Loss: 0.7399488091468811\n",
      "Iteration: 53, Loss: 0.657073974609375\n",
      "Iteration: 54, Loss: 0.9681207537651062\n",
      "Iteration: 55, Loss: 1.020263671875\n",
      "Iteration: 56, Loss: 0.5811296105384827\n",
      "Iteration: 57, Loss: 0.922530472278595\n",
      "Iteration: 58, Loss: 0.7530909776687622\n",
      "Iteration: 59, Loss: 0.917382538318634\n",
      "Iteration: 60, Loss: 0.7011864185333252\n",
      "Iteration: 61, Loss: 0.6900317668914795\n",
      "Iteration: 62, Loss: 0.6800369024276733\n",
      "Iteration: 63, Loss: 1.2534289360046387\n",
      "Iteration: 64, Loss: 0.6152409911155701\n",
      "Iteration: 65, Loss: 0.7217914462089539\n",
      "Iteration: 66, Loss: 0.9580776691436768\n",
      "Iteration: 67, Loss: 0.8601000905036926\n",
      "Iteration: 68, Loss: 0.7461236715316772\n",
      "Iteration: 69, Loss: 0.946772038936615\n",
      "Iteration: 70, Loss: 0.7349383234977722\n",
      "Iteration: 71, Loss: 0.7101752758026123\n",
      "Iteration: 72, Loss: 0.6816136240959167\n",
      "Iteration: 73, Loss: 0.6807287931442261\n",
      "Iteration: 74, Loss: 0.5876947045326233\n",
      "Iteration: 75, Loss: 1.0342316627502441\n",
      "Iteration: 76, Loss: 0.7744210362434387\n",
      "Iteration: 77, Loss: 0.7396919131278992\n",
      "Iteration: 78, Loss: 0.795811653137207\n",
      "Iteration: 79, Loss: 0.8012292385101318\n",
      "Iteration: 80, Loss: 0.6524122953414917\n",
      "Iteration: 81, Loss: 0.8338672518730164\n",
      "Iteration: 82, Loss: 0.6850680112838745\n",
      "Iteration: 83, Loss: 0.6265398859977722\n",
      "Iteration: 84, Loss: 1.0885918140411377\n",
      "Iteration: 85, Loss: 1.229202151298523\n",
      "Iteration: 86, Loss: 0.6643394827842712\n",
      "Iteration: 87, Loss: 0.7612617611885071\n",
      "Iteration: 88, Loss: 0.6047794222831726\n",
      "Iteration: 89, Loss: 1.0467551946640015\n",
      "Iteration: 90, Loss: 0.7792713046073914\n",
      "Iteration: 91, Loss: 0.6397411227226257\n",
      "Iteration: 92, Loss: 0.7889343500137329\n",
      "Iteration: 93, Loss: 0.9203559160232544\n",
      "Iteration: 94, Loss: 0.9453628063201904\n",
      "Iteration: 95, Loss: 0.7161046266555786\n",
      "Iteration: 96, Loss: 0.6237838268280029\n",
      "Iteration: 97, Loss: 0.636454701423645\n",
      "Iteration: 98, Loss: 0.8315309286117554\n",
      "Iteration: 99, Loss: 0.7511199116706848\n",
      "Iteration: 100, Loss: 0.9967018365859985\n",
      "Iteration: 101, Loss: 1.2021164894104004\n",
      "Iteration: 102, Loss: 0.603015124797821\n",
      "Iteration: 103, Loss: 0.6259471774101257\n",
      "Iteration: 104, Loss: 0.6617218852043152\n",
      "Iteration: 105, Loss: 0.6772071719169617\n",
      "Iteration: 106, Loss: 1.0162919759750366\n",
      "Iteration: 107, Loss: 0.7270618081092834\n",
      "Iteration: 108, Loss: 0.920365571975708\n",
      "Iteration: 109, Loss: 0.6657047867774963\n",
      "Iteration: 110, Loss: 0.9591471552848816\n",
      "Iteration: 111, Loss: 0.6572484970092773\n",
      "Iteration: 112, Loss: 0.8783822655677795\n",
      "Iteration: 113, Loss: 0.772255003452301\n",
      "Iteration: 114, Loss: 0.8814471960067749\n",
      "Iteration: 115, Loss: 0.6311362385749817\n",
      "Iteration: 116, Loss: 0.6692418456077576\n",
      "Iteration: 117, Loss: 0.6697594523429871\n",
      "Iteration: 118, Loss: 0.8003767132759094\n",
      "Iteration: 119, Loss: 0.704014778137207\n",
      "Iteration: 120, Loss: 0.7854090929031372\n",
      "Iteration: 121, Loss: 0.7710873484611511\n",
      "Iteration: 122, Loss: 0.7132688760757446\n",
      "Iteration: 123, Loss: 0.7338641285896301\n",
      "Iteration: 124, Loss: 0.6263788938522339\n",
      "Iteration: 125, Loss: 0.6284072995185852\n",
      "Iteration: 126, Loss: 0.7554839849472046\n",
      "Iteration: 127, Loss: 0.7341034412384033\n",
      "Iteration: 128, Loss: 0.6918211579322815\n",
      "Iteration: 129, Loss: 0.6833114624023438\n",
      "Iteration: 130, Loss: 0.7062361836433411\n",
      "Iteration: 131, Loss: 0.6483203172683716\n",
      "Iteration: 132, Loss: 0.7009152770042419\n",
      "Iteration: 133, Loss: 0.613778829574585\n",
      "Iteration: 134, Loss: 0.7172979712486267\n",
      "Iteration: 135, Loss: 0.7860330939292908\n",
      "Iteration: 136, Loss: 0.6530832648277283\n",
      "Iteration: 137, Loss: 0.612038791179657\n",
      "Iteration: 138, Loss: 0.811505913734436\n",
      "Iteration: 139, Loss: 0.6816530227661133\n",
      "Iteration: 140, Loss: 0.7885218262672424\n",
      "Iteration: 141, Loss: 0.6547354459762573\n",
      "Iteration: 142, Loss: 0.7298621535301208\n",
      "Iteration: 143, Loss: 0.7820819616317749\n",
      "Iteration: 144, Loss: 0.6915829181671143\n",
      "Iteration: 145, Loss: 0.5703678131103516\n",
      "Iteration: 146, Loss: 0.6926428079605103\n",
      "Iteration: 147, Loss: 0.93210768699646\n",
      "Iteration: 148, Loss: 0.69664067029953\n",
      "Iteration: 149, Loss: 0.7192127108573914\n",
      "Iteration: 150, Loss: 0.5659482479095459\n",
      "Iteration: 151, Loss: 0.6679723262786865\n",
      "Iteration: 152, Loss: 0.773396372795105\n",
      "Iteration: 153, Loss: 0.6429341435432434\n",
      "Iteration: 154, Loss: 0.7525754570960999\n",
      "Iteration: 155, Loss: 0.7131657004356384\n",
      "Iteration: 156, Loss: 0.6200941205024719\n",
      "Iteration: 157, Loss: 0.6894802451133728\n",
      "Iteration: 158, Loss: 0.6600443124771118\n",
      "Iteration: 159, Loss: 0.6618478298187256\n",
      "Iteration: 160, Loss: 0.6579766869544983\n",
      "Iteration: 161, Loss: 0.6912168860435486\n",
      "Iteration: 162, Loss: 0.764864981174469\n",
      "Iteration: 163, Loss: 0.6410019397735596\n",
      "Iteration: 164, Loss: 0.6450462341308594\n",
      "Iteration: 165, Loss: 0.6550926566123962\n",
      "Iteration: 166, Loss: 0.7449460625648499\n",
      "Iteration: 167, Loss: 0.6131347417831421\n",
      "Iteration: 168, Loss: 0.6370521783828735\n",
      "Iteration: 169, Loss: 0.6735602617263794\n",
      "Iteration: 170, Loss: 0.7021673321723938\n",
      "Iteration: 171, Loss: 0.6468353271484375\n",
      "Iteration: 172, Loss: 0.6377920508384705\n",
      "Iteration: 173, Loss: 0.6770713329315186\n",
      "Iteration: 174, Loss: 0.5782176852226257\n",
      "Iteration: 175, Loss: 0.722634494304657\n",
      "Iteration: 176, Loss: 0.6148384213447571\n",
      "Iteration: 177, Loss: 0.6363548636436462\n",
      "Iteration: 178, Loss: 0.7277146577835083\n",
      "Iteration: 179, Loss: 0.7401716709136963\n",
      "Iteration: 180, Loss: 0.6055338978767395\n",
      "Iteration: 181, Loss: 0.7221487760543823\n",
      "Iteration: 182, Loss: 0.6403278112411499\n",
      "Iteration: 183, Loss: 0.7698238492012024\n",
      "Iteration: 184, Loss: 0.610456109046936\n",
      "Iteration: 185, Loss: 0.7232008576393127\n",
      "Iteration: 186, Loss: 0.7045494318008423\n",
      "Iteration: 187, Loss: 0.649662971496582\n",
      "Iteration: 188, Loss: 0.6082751750946045\n",
      "Iteration: 189, Loss: 0.6270423531532288\n",
      "Iteration: 190, Loss: 0.7047576308250427\n",
      "Iteration: 191, Loss: 0.6507083773612976\n",
      "Iteration: 192, Loss: 0.6514256000518799\n",
      "Iteration: 193, Loss: 0.6002112030982971\n",
      "Iteration: 194, Loss: 0.691624104976654\n",
      "Iteration: 195, Loss: 0.6158379912376404\n",
      "Iteration: 196, Loss: 0.6710952520370483\n",
      "Iteration: 197, Loss: 0.579952597618103\n",
      "Iteration: 198, Loss: 0.6202293634414673\n",
      "Iteration: 199, Loss: 0.6769884824752808\n",
      "Iteration: 200, Loss: 0.6725218892097473\n",
      "Iteration: 201, Loss: 0.5567664504051208\n",
      "Iteration: 202, Loss: 0.6529405117034912\n",
      "Iteration: 203, Loss: 0.6548213362693787\n",
      "Iteration: 204, Loss: 0.6797149181365967\n",
      "Iteration: 205, Loss: 0.5915642380714417\n",
      "Iteration: 206, Loss: 0.645891010761261\n",
      "Iteration: 207, Loss: 0.6136847138404846\n",
      "Iteration: 208, Loss: 0.7021306157112122\n",
      "Iteration: 209, Loss: 0.6818003058433533\n",
      "Iteration: 210, Loss: 0.5695152878761292\n",
      "Iteration: 211, Loss: 0.5896987915039062\n",
      "Iteration: 212, Loss: 0.7082865834236145\n",
      "Iteration: 213, Loss: 0.6599931120872498\n",
      "Iteration: 214, Loss: 0.5500567555427551\n",
      "Iteration: 215, Loss: 0.6801732778549194\n",
      "Iteration: 216, Loss: 0.6914323568344116\n",
      "Iteration: 217, Loss: 0.5876893401145935\n",
      "Iteration: 218, Loss: 0.6679163575172424\n",
      "Iteration: 219, Loss: 0.612796425819397\n",
      "Iteration: 220, Loss: 0.6226360201835632\n",
      "Iteration: 221, Loss: 0.590903639793396\n",
      "Iteration: 222, Loss: 0.6407573223114014\n",
      "Iteration: 223, Loss: 0.6169888973236084\n",
      "Iteration: 224, Loss: 0.6621504426002502\n",
      "Iteration: 225, Loss: 0.5649977326393127\n",
      "Iteration: 226, Loss: 0.6226032376289368\n",
      "Iteration: 227, Loss: 0.5766856670379639\n",
      "Iteration: 228, Loss: 0.6556224226951599\n",
      "Iteration: 229, Loss: 0.7279671430587769\n",
      "Iteration: 230, Loss: 0.619693398475647\n",
      "Iteration: 231, Loss: 0.5856004357337952\n",
      "Iteration: 232, Loss: 0.5492727160453796\n",
      "Iteration: 233, Loss: 0.5658991932868958\n",
      "Iteration: 234, Loss: 0.6304633021354675\n",
      "Iteration: 235, Loss: 0.6686210632324219\n",
      "Iteration: 236, Loss: 0.5879422426223755\n",
      "Iteration: 237, Loss: 0.5962438583374023\n",
      "Iteration: 238, Loss: 0.6736337542533875\n",
      "Iteration: 239, Loss: 0.5153401494026184\n",
      "Iteration: 240, Loss: 0.5920629501342773\n",
      "Iteration: 241, Loss: 0.5590562224388123\n",
      "Iteration: 242, Loss: 0.6706329584121704\n",
      "Iteration: 243, Loss: 0.5602084994316101\n",
      "Iteration: 244, Loss: 0.7250819802284241\n",
      "Iteration: 245, Loss: 0.5109789371490479\n",
      "Iteration: 246, Loss: 0.6016885638237\n",
      "Iteration: 247, Loss: 0.6445409059524536\n",
      "Iteration: 248, Loss: 0.6452977657318115\n",
      "Iteration: 249, Loss: 0.5887515544891357\n",
      "Iteration: 250, Loss: 0.5612853765487671\n",
      "Iteration: 251, Loss: 0.6359062194824219\n",
      "Iteration: 252, Loss: 0.6428329944610596\n",
      "Iteration: 253, Loss: 0.6754249930381775\n",
      "Iteration: 254, Loss: 0.4619966447353363\n",
      "Iteration: 255, Loss: 0.642570436000824\n",
      "Iteration: 256, Loss: 0.6562731266021729\n",
      "Iteration: 257, Loss: 0.565779983997345\n",
      "Iteration: 258, Loss: 0.6363773345947266\n",
      "Iteration: 259, Loss: 0.5392488837242126\n",
      "Iteration: 260, Loss: 0.6236189603805542\n",
      "Iteration: 261, Loss: 0.6274439692497253\n",
      "Iteration: 262, Loss: 0.5983313918113708\n",
      "Iteration: 263, Loss: 0.6235074996948242\n",
      "Iteration: 264, Loss: 0.5520657300949097\n",
      "Iteration: 265, Loss: 0.5811206102371216\n",
      "Iteration: 266, Loss: 0.5906897187232971\n",
      "Iteration: 267, Loss: 0.5368167757987976\n",
      "Iteration: 268, Loss: 0.6417677402496338\n",
      "Iteration: 269, Loss: 0.6028935313224792\n",
      "Iteration: 270, Loss: 0.5703235864639282\n",
      "Iteration: 271, Loss: 0.5872839689254761\n",
      "Iteration: 272, Loss: 0.572360098361969\n",
      "Iteration: 273, Loss: 0.520612359046936\n",
      "Iteration: 274, Loss: 0.544880211353302\n",
      "Iteration: 275, Loss: 0.6410819292068481\n",
      "Iteration: 276, Loss: 0.6280972957611084\n",
      "Iteration: 277, Loss: 0.521098792552948\n",
      "Iteration: 278, Loss: 0.5660200715065002\n",
      "Iteration: 279, Loss: 0.6357580423355103\n",
      "Iteration: 280, Loss: 0.6323052644729614\n",
      "Iteration: 281, Loss: 0.5606116652488708\n",
      "Iteration: 282, Loss: 0.5646176934242249\n",
      "Iteration: 283, Loss: 0.538964033126831\n",
      "Iteration: 284, Loss: 0.7279810309410095\n",
      "Iteration: 285, Loss: 0.6324055194854736\n",
      "Iteration: 286, Loss: 0.6325522065162659\n",
      "Iteration: 287, Loss: 0.5157955884933472\n",
      "Iteration: 288, Loss: 0.5989474058151245\n",
      "Iteration: 289, Loss: 0.5823697447776794\n",
      "Iteration: 290, Loss: 0.601082444190979\n",
      "Iteration: 291, Loss: 0.5939218997955322\n",
      "Iteration: 292, Loss: 0.5885096788406372\n",
      "Iteration: 293, Loss: 0.6069262027740479\n",
      "Iteration: 294, Loss: 0.5838324427604675\n",
      "Iteration: 295, Loss: 0.5755259990692139\n",
      "Iteration: 296, Loss: 0.5832967162132263\n",
      "Iteration: 297, Loss: 0.6257103681564331\n",
      "Iteration: 298, Loss: 0.5672956109046936\n",
      "Iteration: 299, Loss: 0.540947675704956\n",
      "Iteration: 300, Loss: 0.615712583065033\n",
      "Iteration: 301, Loss: 0.6486985683441162\n",
      "Iteration: 302, Loss: 0.5272454619407654\n",
      "Iteration: 303, Loss: 0.619864284992218\n",
      "Iteration: 304, Loss: 0.6091709136962891\n",
      "Iteration: 305, Loss: 0.556268572807312\n",
      "Iteration: 306, Loss: 0.617791473865509\n",
      "Iteration: 307, Loss: 0.6310818195343018\n",
      "Iteration: 308, Loss: 0.5282108783721924\n",
      "Iteration: 309, Loss: 0.5684815645217896\n",
      "Iteration: 310, Loss: 0.579300582408905\n",
      "Iteration: 311, Loss: 0.6582700610160828\n",
      "Iteration: 312, Loss: 0.5291491746902466\n",
      "Iteration: 313, Loss: 0.5844088196754456\n",
      "Iteration: 314, Loss: 0.6587736010551453\n",
      "Iteration: 315, Loss: 0.5797718167304993\n",
      "Iteration: 316, Loss: 0.5416520237922668\n",
      "Iteration: 317, Loss: 0.6367343664169312\n",
      "Iteration: 318, Loss: 0.5484945178031921\n",
      "Iteration: 319, Loss: 0.6398722529411316\n",
      "Iteration: 320, Loss: 0.5533716082572937\n",
      "Iteration: 321, Loss: 0.5441307425498962\n",
      "Iteration: 322, Loss: 0.5465667843818665\n",
      "Iteration: 323, Loss: 0.6383807063102722\n",
      "Iteration: 324, Loss: 0.6229641437530518\n",
      "Iteration: 325, Loss: 0.6335615515708923\n",
      "Iteration: 326, Loss: 0.49785277247428894\n",
      "Iteration: 327, Loss: 0.615887463092804\n",
      "Iteration: 328, Loss: 0.6240249276161194\n",
      "Iteration: 329, Loss: 0.4446948766708374\n",
      "Iteration: 330, Loss: 0.6123415231704712\n",
      "Iteration: 331, Loss: 0.6049813628196716\n",
      "Iteration: 332, Loss: 0.7067498564720154\n",
      "Iteration: 333, Loss: 0.5744865536689758\n",
      "Iteration: 334, Loss: 0.608605682849884\n",
      "Iteration: 335, Loss: 0.537756621837616\n",
      "Iteration: 336, Loss: 0.6174226999282837\n",
      "Iteration: 337, Loss: 0.5568439364433289\n",
      "Iteration: 338, Loss: 0.6809476017951965\n",
      "Iteration: 339, Loss: 0.5406983494758606\n",
      "Iteration: 340, Loss: 0.5626422762870789\n",
      "Iteration: 341, Loss: 0.5263675451278687\n",
      "Iteration: 342, Loss: 0.5232630372047424\n",
      "Iteration: 343, Loss: 0.6752100586891174\n",
      "Iteration: 344, Loss: 0.6702964305877686\n",
      "Iteration: 345, Loss: 0.5990683436393738\n",
      "Iteration: 346, Loss: 0.6027241349220276\n",
      "Iteration: 347, Loss: 0.5133081078529358\n",
      "Iteration: 348, Loss: 0.5785808563232422\n",
      "Iteration: 349, Loss: 0.6137353777885437\n",
      "Iteration: 350, Loss: 0.6311255097389221\n",
      "Iteration: 351, Loss: 0.553810179233551\n",
      "Iteration: 352, Loss: 0.5490151643753052\n",
      "Iteration: 353, Loss: 0.5603521466255188\n",
      "Iteration: 354, Loss: 0.6086256504058838\n",
      "Iteration: 355, Loss: 0.5371447205543518\n",
      "Iteration: 356, Loss: 0.6013035178184509\n",
      "Iteration: 357, Loss: 0.5526320338249207\n",
      "Iteration: 358, Loss: 0.6380571722984314\n",
      "Iteration: 359, Loss: 0.5651804804801941\n",
      "Iteration: 360, Loss: 0.6006117463111877\n",
      "Iteration: 361, Loss: 0.583722710609436\n",
      "Iteration: 362, Loss: 0.5558252334594727\n",
      "Iteration: 363, Loss: 0.5669226050376892\n",
      "Iteration: 364, Loss: 0.6054667830467224\n",
      "Iteration: 365, Loss: 0.5919171571731567\n",
      "Iteration: 366, Loss: 0.6115599870681763\n",
      "Iteration: 367, Loss: 0.49386805295944214\n",
      "Iteration: 368, Loss: 0.6263179779052734\n",
      "Iteration: 369, Loss: 0.5232667326927185\n",
      "Iteration: 370, Loss: 0.5943039059638977\n",
      "Iteration: 371, Loss: 0.5300083756446838\n",
      "Iteration: 372, Loss: 0.6460320353507996\n",
      "Iteration: 373, Loss: 0.5807095766067505\n",
      "Iteration: 374, Loss: 0.5423901081085205\n",
      "Iteration: 375, Loss: 0.59022456407547\n",
      "Iteration: 376, Loss: 0.5892007946968079\n",
      "Iteration: 377, Loss: 0.5533733367919922\n",
      "Iteration: 378, Loss: 0.5904053449630737\n",
      "Iteration: 379, Loss: 0.6427624225616455\n",
      "Iteration: 380, Loss: 0.5699723362922668\n",
      "Iteration: 381, Loss: 0.5014347434043884\n",
      "Iteration: 382, Loss: 0.5833495259284973\n",
      "Iteration: 383, Loss: 0.5814116597175598\n",
      "Iteration: 384, Loss: 0.646053671836853\n",
      "Iteration: 385, Loss: 0.5093213319778442\n",
      "Iteration: 386, Loss: 0.6701739430427551\n",
      "Iteration: 387, Loss: 0.6132392287254333\n",
      "Iteration: 388, Loss: 0.5459194183349609\n",
      "Iteration: 389, Loss: 0.6093575358390808\n",
      "Iteration: 390, Loss: 0.5799529552459717\n",
      "Iteration: 391, Loss: 0.6331338286399841\n",
      "Iteration: 392, Loss: 0.5393722653388977\n",
      "Iteration: 393, Loss: 0.6142996549606323\n",
      "Iteration: 394, Loss: 0.5291251540184021\n",
      "Iteration: 395, Loss: 0.5749655365943909\n",
      "Iteration: 396, Loss: 0.5983953475952148\n",
      "Iteration: 397, Loss: 0.5757981538772583\n",
      "Iteration: 398, Loss: 0.5494557023048401\n",
      "Iteration: 399, Loss: 0.5494970679283142\n",
      "Iteration: 400, Loss: 0.6119519472122192\n",
      "Iteration: 401, Loss: 0.6264072060585022\n",
      "Iteration: 402, Loss: 0.6302124261856079\n",
      "Iteration: 403, Loss: 0.5591800212860107\n",
      "Iteration: 404, Loss: 0.48808619379997253\n",
      "Iteration: 405, Loss: 0.6358833312988281\n",
      "Iteration: 406, Loss: 0.4976963698863983\n",
      "Iteration: 407, Loss: 0.5804731249809265\n",
      "Iteration: 408, Loss: 0.6243866086006165\n",
      "Iteration: 409, Loss: 0.662375271320343\n",
      "Iteration: 410, Loss: 0.5725269317626953\n",
      "Iteration: 411, Loss: 0.4975457191467285\n",
      "Iteration: 412, Loss: 0.555138349533081\n",
      "Iteration: 413, Loss: 0.5356339812278748\n",
      "Iteration: 414, Loss: 0.586530327796936\n",
      "Iteration: 415, Loss: 0.616992175579071\n",
      "Iteration: 416, Loss: 0.5242295861244202\n",
      "Iteration: 417, Loss: 0.6274935603141785\n",
      "Iteration: 418, Loss: 0.5474379658699036\n",
      "Iteration: 419, Loss: 0.49659600853919983\n",
      "Iteration: 420, Loss: 0.6396359801292419\n",
      "Iteration: 421, Loss: 0.6124292016029358\n",
      "Iteration: 422, Loss: 0.48070186376571655\n",
      "Iteration: 423, Loss: 0.6067413091659546\n",
      "Iteration: 424, Loss: 0.5730056762695312\n",
      "Iteration: 425, Loss: 0.5624533295631409\n",
      "Iteration: 426, Loss: 0.6111956834793091\n",
      "Iteration: 427, Loss: 0.5271912217140198\n",
      "Iteration: 428, Loss: 0.5628284811973572\n",
      "Iteration: 429, Loss: 0.5991271734237671\n",
      "Iteration: 430, Loss: 0.6028150916099548\n",
      "Iteration: 431, Loss: 0.5625466108322144\n",
      "Iteration: 432, Loss: 0.5199647545814514\n",
      "Iteration: 433, Loss: 0.5695048570632935\n",
      "Iteration: 434, Loss: 0.5360345244407654\n",
      "Iteration: 435, Loss: 0.6006143689155579\n",
      "Iteration: 436, Loss: 0.5486180782318115\n",
      "Iteration: 437, Loss: 0.5646752715110779\n",
      "Iteration: 438, Loss: 0.5704969763755798\n",
      "Iteration: 439, Loss: 0.5271733403205872\n",
      "Iteration: 440, Loss: 0.5770483613014221\n",
      "Iteration: 441, Loss: 0.6015205383300781\n",
      "Iteration: 442, Loss: 0.5831555724143982\n",
      "Iteration: 443, Loss: 0.5179486274719238\n",
      "Iteration: 444, Loss: 0.5860417485237122\n",
      "Iteration: 445, Loss: 0.5796211957931519\n",
      "Iteration: 446, Loss: 0.4626717269420624\n",
      "Iteration: 447, Loss: 0.6422244906425476\n",
      "Iteration: 448, Loss: 0.5605500340461731\n",
      "Iteration: 449, Loss: 0.5763601660728455\n",
      "Iteration: 450, Loss: 0.6138366460800171\n",
      "Iteration: 451, Loss: 0.5461454391479492\n",
      "Iteration: 452, Loss: 0.5109317898750305\n",
      "Iteration: 453, Loss: 0.5394576191902161\n",
      "Iteration: 454, Loss: 0.5783641338348389\n",
      "Iteration: 455, Loss: 0.6049568057060242\n",
      "Iteration: 456, Loss: 0.5296215415000916\n",
      "Iteration: 457, Loss: 0.5107085704803467\n",
      "Iteration: 458, Loss: 0.5930967330932617\n",
      "Iteration: 459, Loss: 0.5623743534088135\n",
      "Iteration: 460, Loss: 0.5787534117698669\n",
      "Iteration: 461, Loss: 0.5284541845321655\n",
      "Iteration: 462, Loss: 0.5816441774368286\n",
      "Iteration: 463, Loss: 0.6134853959083557\n",
      "Iteration: 464, Loss: 0.5279054641723633\n",
      "Iteration: 465, Loss: 0.5972573161125183\n",
      "Iteration: 466, Loss: 0.5069332122802734\n",
      "Iteration: 467, Loss: 0.5804115533828735\n",
      "Iteration: 468, Loss: 0.5541499853134155\n",
      "Iteration: 469, Loss: 0.6001046299934387\n",
      "Iteration: 470, Loss: 0.5587244629859924\n",
      "Iteration: 471, Loss: 0.5348203182220459\n",
      "Iteration: 472, Loss: 0.5270378589630127\n",
      "Iteration: 473, Loss: 0.5771969556808472\n",
      "Iteration: 474, Loss: 0.6253349184989929\n",
      "Iteration: 475, Loss: 0.4854707717895508\n",
      "Iteration: 476, Loss: 0.5992993712425232\n",
      "Iteration: 477, Loss: 0.5562242865562439\n",
      "Iteration: 478, Loss: 0.577935516834259\n",
      "Iteration: 479, Loss: 0.5800848603248596\n",
      "Iteration: 480, Loss: 0.5491484999656677\n",
      "Iteration: 481, Loss: 0.4800349473953247\n",
      "Iteration: 482, Loss: 0.6429044604301453\n",
      "Iteration: 483, Loss: 0.5884513854980469\n",
      "Iteration: 484, Loss: 0.5975339412689209\n",
      "Iteration: 485, Loss: 0.47847989201545715\n",
      "Iteration: 486, Loss: 0.6553910374641418\n",
      "Iteration: 487, Loss: 0.5483642816543579\n",
      "Iteration: 488, Loss: 0.5536595582962036\n",
      "Iteration: 489, Loss: 0.5737596750259399\n",
      "Iteration: 490, Loss: 0.5824689269065857\n",
      "Iteration: 491, Loss: 0.6084528565406799\n",
      "Iteration: 492, Loss: 0.47462716698646545\n",
      "Iteration: 493, Loss: 0.6026986241340637\n",
      "Iteration: 494, Loss: 0.5106099843978882\n",
      "Iteration: 495, Loss: 0.5773835778236389\n",
      "Iteration: 496, Loss: 0.5516853332519531\n",
      "Iteration: 497, Loss: 0.5444304347038269\n",
      "Iteration: 498, Loss: 0.5357123017311096\n",
      "Iteration: 499, Loss: 0.5258490443229675\n",
      "Iteration: 500, Loss: 0.5852213501930237\n",
      "Iteration: 501, Loss: 0.5451333522796631\n",
      "Iteration: 502, Loss: 0.644003689289093\n",
      "Iteration: 503, Loss: 0.44691160321235657\n",
      "Iteration: 504, Loss: 0.6114723682403564\n",
      "Iteration: 505, Loss: 0.49723973870277405\n",
      "Iteration: 506, Loss: 0.5504983067512512\n",
      "Iteration: 507, Loss: 0.5629962086677551\n",
      "Iteration: 508, Loss: 0.7012993693351746\n",
      "Iteration: 509, Loss: 0.512241005897522\n",
      "Iteration: 510, Loss: 0.6220783591270447\n",
      "Iteration: 511, Loss: 0.5601354837417603\n",
      "Iteration: 512, Loss: 0.5809838175773621\n",
      "Iteration: 513, Loss: 0.5776285529136658\n",
      "Iteration: 514, Loss: 0.6838617920875549\n",
      "Iteration: 515, Loss: 0.5083163976669312\n",
      "Iteration: 516, Loss: 0.5485026836395264\n",
      "Iteration: 517, Loss: 0.6508022546768188\n",
      "Iteration: 518, Loss: 0.5454825162887573\n",
      "Iteration: 519, Loss: 0.5583488345146179\n",
      "Iteration: 520, Loss: 0.5990378260612488\n",
      "Iteration: 521, Loss: 0.519687831401825\n",
      "Iteration: 522, Loss: 0.628544807434082\n",
      "Iteration: 523, Loss: 0.5570639967918396\n",
      "Iteration: 524, Loss: 0.5419610738754272\n",
      "Iteration: 525, Loss: 0.6379280090332031\n",
      "Iteration: 526, Loss: 0.5535658001899719\n",
      "Iteration: 527, Loss: 0.5433127880096436\n",
      "Iteration: 528, Loss: 0.5331047773361206\n",
      "Iteration: 529, Loss: 0.5836589336395264\n",
      "Iteration: 530, Loss: 0.5307249426841736\n",
      "Iteration: 531, Loss: 0.6170821785926819\n",
      "Iteration: 532, Loss: 0.5939886569976807\n",
      "Iteration: 533, Loss: 0.5245416760444641\n",
      "Iteration: 534, Loss: 0.5875833630561829\n",
      "Iteration: 535, Loss: 0.5630661845207214\n",
      "Iteration: 536, Loss: 0.5629990696907043\n",
      "Iteration: 537, Loss: 0.5013582110404968\n",
      "Iteration: 538, Loss: 0.6218796968460083\n",
      "Iteration: 539, Loss: 0.54727703332901\n",
      "Iteration: 540, Loss: 0.5510543584823608\n",
      "Iteration: 541, Loss: 0.5276155471801758\n",
      "Iteration: 542, Loss: 0.5439544320106506\n",
      "Iteration: 543, Loss: 0.5814843773841858\n",
      "Iteration: 544, Loss: 0.5639564990997314\n",
      "Iteration: 545, Loss: 0.5058122277259827\n",
      "Iteration: 546, Loss: 0.5230181813240051\n",
      "Iteration: 547, Loss: 0.5710678696632385\n",
      "Iteration: 548, Loss: 0.5970180034637451\n",
      "Iteration: 549, Loss: 0.6076138019561768\n",
      "Iteration: 550, Loss: 0.6152799129486084\n",
      "Iteration: 551, Loss: 0.511962354183197\n",
      "Iteration: 552, Loss: 0.5235108733177185\n",
      "Iteration: 553, Loss: 0.5052258968353271\n",
      "Iteration: 554, Loss: 0.569774866104126\n",
      "Iteration: 555, Loss: 0.5972656011581421\n",
      "Iteration: 556, Loss: 0.5143721103668213\n",
      "Iteration: 557, Loss: 0.5552892684936523\n",
      "Iteration: 558, Loss: 0.5771361589431763\n",
      "Iteration: 559, Loss: 0.5375655889511108\n",
      "Iteration: 560, Loss: 0.4993186295032501\n",
      "Iteration: 561, Loss: 0.5387442111968994\n",
      "Iteration: 562, Loss: 0.5784428119659424\n",
      "Iteration: 563, Loss: 0.5424593091011047\n",
      "Iteration: 564, Loss: 0.5581017732620239\n",
      "Iteration: 565, Loss: 0.554890513420105\n",
      "Iteration: 566, Loss: 0.5731680393218994\n",
      "Iteration: 567, Loss: 0.548798143863678\n",
      "Iteration: 568, Loss: 0.5057229995727539\n",
      "Iteration: 569, Loss: 0.6313562393188477\n",
      "Iteration: 570, Loss: 0.5731079578399658\n",
      "Iteration: 571, Loss: 0.5131470561027527\n",
      "Iteration: 572, Loss: 0.49284106492996216\n",
      "Iteration: 573, Loss: 0.5219957828521729\n",
      "Iteration: 574, Loss: 0.5767032504081726\n",
      "Iteration: 575, Loss: 0.5679117441177368\n",
      "Iteration: 576, Loss: 0.6123988628387451\n",
      "Iteration: 577, Loss: 0.4485694468021393\n",
      "Iteration: 578, Loss: 0.5789206027984619\n",
      "Iteration: 579, Loss: 0.5674765110015869\n",
      "Iteration: 580, Loss: 0.6265344023704529\n",
      "Iteration: 581, Loss: 0.5514451265335083\n",
      "Iteration: 582, Loss: 0.5399845838546753\n",
      "Iteration: 583, Loss: 0.6141700148582458\n",
      "Iteration: 584, Loss: 0.5318719744682312\n",
      "Iteration: 585, Loss: 0.6298379898071289\n",
      "Iteration: 586, Loss: 0.5020677447319031\n",
      "Iteration: 587, Loss: 0.5733053684234619\n",
      "Iteration: 588, Loss: 0.5450284481048584\n",
      "Iteration: 589, Loss: 0.5300989747047424\n",
      "Iteration: 590, Loss: 0.6153717041015625\n",
      "Iteration: 591, Loss: 0.6399315595626831\n",
      "Iteration: 592, Loss: 0.5015280246734619\n",
      "Iteration: 593, Loss: 0.6025657653808594\n",
      "Iteration: 594, Loss: 0.5717589855194092\n",
      "Iteration: 595, Loss: 0.5354815125465393\n",
      "Iteration: 596, Loss: 0.5400984883308411\n",
      "Iteration: 597, Loss: 0.5527156591415405\n",
      "Iteration: 598, Loss: 0.5457862019538879\n",
      "Iteration: 599, Loss: 0.664825975894928\n",
      "Iteration: 600, Loss: 0.5054075717926025\n",
      "Iteration: 601, Loss: 0.495323121547699\n",
      "Iteration: 602, Loss: 0.6780328750610352\n",
      "Iteration: 603, Loss: 0.49056074023246765\n",
      "Iteration: 604, Loss: 0.6052542924880981\n",
      "Iteration: 605, Loss: 0.5650659203529358\n",
      "Iteration: 606, Loss: 0.5915876030921936\n",
      "Iteration: 607, Loss: 0.5714580416679382\n",
      "Iteration: 608, Loss: 0.5553770661354065\n",
      "Iteration: 609, Loss: 0.6368986368179321\n",
      "Iteration: 610, Loss: 0.5539347529411316\n",
      "Iteration: 611, Loss: 0.5021674036979675\n",
      "Iteration: 612, Loss: 0.5596681833267212\n",
      "Iteration: 613, Loss: 0.5342750549316406\n",
      "Iteration: 614, Loss: 0.5654228329658508\n",
      "Iteration: 615, Loss: 0.5332369804382324\n",
      "Iteration: 616, Loss: 0.5787270665168762\n",
      "Iteration: 617, Loss: 0.5358928442001343\n",
      "Iteration: 618, Loss: 0.5840479731559753\n",
      "Iteration: 619, Loss: 0.6017569303512573\n",
      "Iteration: 620, Loss: 0.5551449656486511\n",
      "Iteration: 621, Loss: 0.5604733228683472\n",
      "Iteration: 622, Loss: 0.5403669476509094\n",
      "Iteration: 623, Loss: 0.5746855139732361\n",
      "Iteration: 624, Loss: 0.5437694787979126\n",
      "Iteration: 625, Loss: 0.5562149882316589\n",
      "Iteration: 626, Loss: 0.6859999895095825\n",
      "Iteration: 627, Loss: 0.5342198610305786\n",
      "Iteration: 628, Loss: 0.46428805589675903\n",
      "Iteration: 629, Loss: 0.664939820766449\n",
      "Iteration: 630, Loss: 0.5501941442489624\n",
      "Iteration: 631, Loss: 0.4855252504348755\n",
      "Iteration: 632, Loss: 0.5064719915390015\n",
      "Iteration: 633, Loss: 0.507297694683075\n",
      "Iteration: 634, Loss: 0.5343613028526306\n",
      "Iteration: 635, Loss: 0.5776920318603516\n",
      "Iteration: 636, Loss: 0.5642777681350708\n",
      "Iteration: 637, Loss: 0.5588086247444153\n",
      "Iteration: 638, Loss: 0.5240172743797302\n",
      "Iteration: 639, Loss: 0.600407063961029\n",
      "Iteration: 640, Loss: 0.5375572443008423\n",
      "Iteration: 641, Loss: 0.5574425458908081\n",
      "Iteration: 642, Loss: 0.4973239600658417\n",
      "Iteration: 643, Loss: 0.6105920076370239\n",
      "Iteration: 644, Loss: 0.5398892164230347\n",
      "Iteration: 645, Loss: 0.530458927154541\n",
      "Iteration: 646, Loss: 0.5674100518226624\n",
      "Iteration: 647, Loss: 0.5275874137878418\n",
      "Iteration: 648, Loss: 0.5209881663322449\n",
      "Iteration: 649, Loss: 0.5256853103637695\n",
      "Iteration: 650, Loss: 0.4859730005264282\n",
      "Iteration: 651, Loss: 0.543840765953064\n",
      "Iteration: 652, Loss: 0.5759528279304504\n",
      "Iteration: 653, Loss: 0.5293830037117004\n",
      "Iteration: 654, Loss: 0.5329811573028564\n",
      "Iteration: 655, Loss: 0.5747081637382507\n",
      "Iteration: 656, Loss: 0.5393087267875671\n",
      "Iteration: 657, Loss: 0.5257337689399719\n",
      "Iteration: 658, Loss: 0.5749739408493042\n",
      "Iteration: 659, Loss: 0.5353981256484985\n",
      "Iteration: 660, Loss: 0.5844525694847107\n",
      "Iteration: 661, Loss: 0.5431485772132874\n",
      "Iteration: 662, Loss: 0.5407091379165649\n",
      "Iteration: 663, Loss: 0.5723856687545776\n",
      "Iteration: 664, Loss: 0.523443877696991\n",
      "Iteration: 665, Loss: 0.6385148763656616\n",
      "Iteration: 666, Loss: 0.49267902970314026\n",
      "Iteration: 667, Loss: 0.5800697803497314\n",
      "Iteration: 668, Loss: 0.4976358115673065\n",
      "Iteration: 669, Loss: 0.5627532601356506\n",
      "Iteration: 670, Loss: 0.5090981721878052\n",
      "Iteration: 671, Loss: 0.540482223033905\n",
      "Iteration: 672, Loss: 0.624812662601471\n",
      "Iteration: 673, Loss: 0.5947391390800476\n",
      "Iteration: 674, Loss: 0.4774303138256073\n",
      "Iteration: 675, Loss: 0.5676504969596863\n",
      "Iteration: 676, Loss: 0.5338959097862244\n",
      "Iteration: 677, Loss: 0.5627970099449158\n",
      "Iteration: 678, Loss: 0.5677524209022522\n",
      "Iteration: 679, Loss: 0.5281471014022827\n",
      "Iteration: 680, Loss: 0.5583502054214478\n",
      "Iteration: 681, Loss: 0.5504423975944519\n",
      "Iteration: 682, Loss: 0.6449069380760193\n",
      "Iteration: 683, Loss: 0.4902338981628418\n",
      "Iteration: 684, Loss: 0.5284007787704468\n",
      "Iteration: 685, Loss: 0.5511671304702759\n",
      "Iteration: 686, Loss: 0.5347933173179626\n",
      "Iteration: 687, Loss: 0.5902632474899292\n",
      "Iteration: 688, Loss: 0.5116950869560242\n",
      "Iteration: 689, Loss: 0.48878705501556396\n",
      "Iteration: 690, Loss: 0.551132082939148\n",
      "Iteration: 691, Loss: 0.49695032835006714\n",
      "Iteration: 692, Loss: 0.6808744668960571\n",
      "Iteration: 693, Loss: 0.5556761026382446\n",
      "Iteration: 694, Loss: 0.5530915856361389\n",
      "Iteration: 695, Loss: 0.5589274168014526\n",
      "Iteration: 696, Loss: 0.5250361561775208\n",
      "Iteration: 697, Loss: 0.5543591976165771\n",
      "Iteration: 698, Loss: 0.5744144320487976\n",
      "Iteration: 699, Loss: 0.5517082214355469\n",
      "Iteration: 700, Loss: 0.5311353802680969\n",
      "Iteration: 701, Loss: 0.5294075012207031\n",
      "Iteration: 702, Loss: 0.5159243941307068\n",
      "Iteration: 703, Loss: 0.5356778502464294\n",
      "Iteration: 704, Loss: 0.6013757586479187\n",
      "Iteration: 705, Loss: 0.5482476353645325\n",
      "Iteration: 706, Loss: 0.5598272681236267\n",
      "Iteration: 707, Loss: 0.5688063502311707\n",
      "Iteration: 708, Loss: 0.5208504796028137\n",
      "Iteration: 709, Loss: 0.6129079461097717\n",
      "Iteration: 710, Loss: 0.5453528761863708\n",
      "Iteration: 711, Loss: 0.4980376064777374\n",
      "Iteration: 712, Loss: 0.5411408543586731\n",
      "Iteration: 713, Loss: 0.5743428468704224\n",
      "Iteration: 714, Loss: 0.5858960747718811\n",
      "Iteration: 715, Loss: 0.514703631401062\n",
      "Iteration: 716, Loss: 0.5273545384407043\n",
      "Iteration: 717, Loss: 0.5318224430084229\n",
      "Iteration: 718, Loss: 0.5725913643836975\n",
      "Iteration: 719, Loss: 0.49612632393836975\n",
      "Iteration: 720, Loss: 0.5793220400810242\n",
      "Iteration: 721, Loss: 0.5716511011123657\n",
      "Iteration: 722, Loss: 0.5087834596633911\n",
      "Iteration: 723, Loss: 0.4829519987106323\n",
      "Iteration: 724, Loss: 0.5953842401504517\n",
      "Iteration: 725, Loss: 0.4737257659435272\n",
      "Iteration: 726, Loss: 0.48833340406417847\n",
      "Iteration: 727, Loss: 0.623503565788269\n",
      "Iteration: 728, Loss: 0.5730951428413391\n",
      "Iteration: 729, Loss: 0.5833851099014282\n",
      "Iteration: 730, Loss: 0.5094060301780701\n",
      "Iteration: 731, Loss: 0.5927219986915588\n",
      "Iteration: 732, Loss: 0.504036009311676\n",
      "Iteration: 733, Loss: 0.4748729467391968\n",
      "Iteration: 734, Loss: 0.5546624660491943\n",
      "Iteration: 735, Loss: 0.564622163772583\n",
      "Iteration: 736, Loss: 0.5924667119979858\n",
      "Iteration: 737, Loss: 0.5204952955245972\n",
      "Iteration: 738, Loss: 0.5685928463935852\n",
      "Iteration: 739, Loss: 0.533412754535675\n",
      "Iteration: 740, Loss: 0.5865660309791565\n",
      "Iteration: 741, Loss: 0.5168663263320923\n",
      "Iteration: 742, Loss: 0.5857704281806946\n",
      "Iteration: 743, Loss: 0.5466005206108093\n",
      "Iteration: 744, Loss: 0.5669682025909424\n",
      "Iteration: 745, Loss: 0.6249598264694214\n",
      "Iteration: 746, Loss: 0.5342517495155334\n",
      "Iteration: 747, Loss: 0.5223077535629272\n",
      "Iteration: 748, Loss: 0.5398702025413513\n",
      "Iteration: 749, Loss: 0.5907406210899353\n",
      "Iteration: 750, Loss: 0.5143048167228699\n",
      "Iteration: 751, Loss: 0.5983783602714539\n",
      "Iteration: 752, Loss: 0.5300379991531372\n",
      "Iteration: 753, Loss: 0.5945008397102356\n",
      "Iteration: 754, Loss: 0.49215683341026306\n",
      "Iteration: 755, Loss: 0.5689221620559692\n",
      "Iteration: 756, Loss: 0.5394467115402222\n",
      "Iteration: 757, Loss: 0.5479999780654907\n",
      "Iteration: 758, Loss: 0.5153678059577942\n",
      "Iteration: 759, Loss: 0.6306605935096741\n",
      "Iteration: 760, Loss: 0.617977499961853\n",
      "Iteration: 761, Loss: 0.47366511821746826\n",
      "Iteration: 762, Loss: 0.6117834448814392\n",
      "Iteration: 763, Loss: 0.5362702012062073\n",
      "Iteration: 764, Loss: 0.649362325668335\n",
      "Iteration: 765, Loss: 0.5578590035438538\n",
      "Iteration: 766, Loss: 0.5441243052482605\n",
      "Iteration: 767, Loss: 0.564920961856842\n",
      "Iteration: 768, Loss: 0.5348231196403503\n",
      "Iteration: 769, Loss: 0.51014643907547\n",
      "Iteration: 770, Loss: 0.5491142272949219\n",
      "Iteration: 771, Loss: 0.5224208235740662\n",
      "Iteration: 772, Loss: 0.6268302202224731\n",
      "Iteration: 773, Loss: 0.6289708018302917\n",
      "Iteration: 774, Loss: 0.4815998673439026\n",
      "Iteration: 775, Loss: 0.5628650188446045\n",
      "Iteration: 776, Loss: 0.5739758610725403\n",
      "Iteration: 777, Loss: 0.6570852994918823\n",
      "Iteration: 778, Loss: 0.5521519184112549\n",
      "Iteration: 779, Loss: 0.5182867050170898\n",
      "Iteration: 780, Loss: 0.5415317416191101\n",
      "Iteration: 781, Loss: 0.4734326899051666\n",
      "Iteration: 782, Loss: 0.6466816663742065\n",
      "Iteration: 783, Loss: 0.5530041456222534\n",
      "Iteration: 784, Loss: 0.5295822024345398\n",
      "Iteration: 785, Loss: 0.5539398193359375\n",
      "Iteration: 786, Loss: 0.5670157074928284\n",
      "Iteration: 787, Loss: 0.6110703349113464\n",
      "Iteration: 788, Loss: 0.4766477048397064\n",
      "Iteration: 789, Loss: 0.6301088333129883\n",
      "Iteration: 790, Loss: 0.5140834450721741\n",
      "Iteration: 791, Loss: 0.5333658456802368\n",
      "Iteration: 792, Loss: 0.5697957873344421\n",
      "Iteration: 793, Loss: 0.4620673954486847\n",
      "Iteration: 794, Loss: 0.6258205771446228\n",
      "Iteration: 795, Loss: 0.5135501027107239\n",
      "Iteration: 796, Loss: 0.6336772441864014\n",
      "Iteration: 797, Loss: 0.6079870462417603\n",
      "Iteration: 798, Loss: 0.5936033129692078\n",
      "Iteration: 799, Loss: 0.5294919013977051\n",
      "Iteration: 800, Loss: 0.5068660974502563\n",
      "Iteration: 801, Loss: 0.5582441687583923\n",
      "Iteration: 802, Loss: 0.6323179602622986\n",
      "Iteration: 803, Loss: 0.47308868169784546\n",
      "Iteration: 804, Loss: 0.5487488508224487\n",
      "Iteration: 805, Loss: 0.5794012546539307\n",
      "Iteration: 806, Loss: 0.4865318536758423\n",
      "Iteration: 807, Loss: 0.6011760830879211\n",
      "Iteration: 808, Loss: 0.5322989225387573\n",
      "Iteration: 809, Loss: 0.5647444128990173\n",
      "Iteration: 810, Loss: 0.465780645608902\n",
      "Iteration: 811, Loss: 0.5562188625335693\n",
      "Iteration: 812, Loss: 0.5955936908721924\n",
      "Iteration: 813, Loss: 0.5789009928703308\n",
      "Iteration: 814, Loss: 0.5074936747550964\n",
      "Iteration: 815, Loss: 0.5551347732543945\n",
      "Iteration: 816, Loss: 0.5078879594802856\n",
      "Iteration: 817, Loss: 0.5236437916755676\n",
      "Iteration: 818, Loss: 0.47152525186538696\n",
      "Iteration: 819, Loss: 0.6591702699661255\n",
      "Iteration: 820, Loss: 0.610945463180542\n",
      "Iteration: 821, Loss: 0.5078902244567871\n",
      "Iteration: 822, Loss: 0.5484660267829895\n",
      "Iteration: 823, Loss: 0.5597336292266846\n",
      "Iteration: 824, Loss: 0.5830339193344116\n",
      "Iteration: 825, Loss: 0.6047266125679016\n",
      "Iteration: 826, Loss: 0.5183804631233215\n",
      "Iteration: 827, Loss: 0.5529441833496094\n",
      "Iteration: 828, Loss: 0.5191268920898438\n",
      "Iteration: 829, Loss: 0.574154794216156\n",
      "Iteration: 830, Loss: 0.5784176588058472\n",
      "Iteration: 831, Loss: 0.5536901354789734\n",
      "Iteration: 832, Loss: 0.5068830251693726\n",
      "Iteration: 833, Loss: 0.5429127812385559\n",
      "Iteration: 834, Loss: 0.5044468641281128\n",
      "Iteration: 835, Loss: 0.5871502161026001\n",
      "Iteration: 836, Loss: 0.526064395904541\n",
      "Iteration: 837, Loss: 0.4920681118965149\n",
      "Iteration: 838, Loss: 0.5348522067070007\n",
      "Iteration: 839, Loss: 0.6264374256134033\n",
      "Iteration: 840, Loss: 0.5389395952224731\n",
      "Iteration: 841, Loss: 0.5434214472770691\n",
      "Iteration: 842, Loss: 0.4922375977039337\n",
      "Iteration: 843, Loss: 0.6046368479728699\n",
      "Iteration: 844, Loss: 0.5474966764450073\n",
      "Iteration: 845, Loss: 0.5246803760528564\n",
      "Iteration: 846, Loss: 0.6280308365821838\n",
      "Iteration: 847, Loss: 0.5033774971961975\n",
      "Iteration: 848, Loss: 0.506404459476471\n",
      "Iteration: 849, Loss: 0.4935205578804016\n",
      "Iteration: 850, Loss: 0.49799269437789917\n",
      "Iteration: 851, Loss: 0.6722167730331421\n",
      "Iteration: 852, Loss: 0.5853736400604248\n",
      "Iteration: 853, Loss: 0.5864755511283875\n",
      "Iteration: 854, Loss: 0.5815621614456177\n",
      "Iteration: 855, Loss: 0.5376154184341431\n",
      "Iteration: 856, Loss: 0.49447816610336304\n",
      "Iteration: 857, Loss: 0.539988100528717\n",
      "Iteration: 858, Loss: 0.581545889377594\n",
      "Iteration: 859, Loss: 0.5536796450614929\n",
      "Iteration: 860, Loss: 0.5164737701416016\n",
      "Iteration: 861, Loss: 0.49248990416526794\n",
      "Iteration: 862, Loss: 0.5448778867721558\n",
      "Iteration: 863, Loss: 0.5882457494735718\n",
      "Iteration: 864, Loss: 0.5680361986160278\n",
      "Iteration: 865, Loss: 0.4750693440437317\n",
      "Iteration: 866, Loss: 0.4653334617614746\n",
      "Iteration: 867, Loss: 0.5277085304260254\n",
      "Iteration: 868, Loss: 0.6849661469459534\n",
      "Iteration: 869, Loss: 0.519608199596405\n",
      "Iteration: 870, Loss: 0.5887478590011597\n",
      "Iteration: 871, Loss: 0.46849092841148376\n",
      "Iteration: 872, Loss: 0.6111236214637756\n",
      "Iteration: 873, Loss: 0.5771068930625916\n",
      "Iteration: 874, Loss: 0.5185396075248718\n",
      "Iteration: 875, Loss: 0.5231868028640747\n",
      "Iteration: 876, Loss: 0.5764769315719604\n",
      "Iteration: 877, Loss: 0.6302955746650696\n",
      "Iteration: 878, Loss: 0.45472967624664307\n",
      "Iteration: 879, Loss: 0.4971238076686859\n",
      "Iteration: 880, Loss: 0.5528737902641296\n",
      "Iteration: 881, Loss: 0.5739381909370422\n",
      "Iteration: 882, Loss: 0.5441051721572876\n",
      "Iteration: 883, Loss: 0.5274839401245117\n",
      "Iteration: 884, Loss: 0.5133936405181885\n",
      "Iteration: 885, Loss: 0.5468158721923828\n",
      "Iteration: 886, Loss: 0.5610125064849854\n",
      "Iteration: 887, Loss: 0.5323371291160583\n",
      "Iteration: 888, Loss: 0.5782398581504822\n",
      "Iteration: 889, Loss: 0.6210268139839172\n",
      "Iteration: 890, Loss: 0.5028458833694458\n",
      "Iteration: 891, Loss: 0.5221304297447205\n",
      "Iteration: 892, Loss: 0.5131798982620239\n",
      "Iteration: 893, Loss: 0.540409505367279\n",
      "Iteration: 894, Loss: 0.5551270842552185\n",
      "Iteration: 895, Loss: 0.5750759243965149\n",
      "Iteration: 896, Loss: 0.49099552631378174\n",
      "Iteration: 897, Loss: 0.48311835527420044\n",
      "Iteration: 898, Loss: 0.4830019772052765\n",
      "Iteration: 899, Loss: 0.5908179879188538\n",
      "Iteration: 900, Loss: 0.6417986750602722\n",
      "Iteration: 901, Loss: 0.5342747569084167\n",
      "Iteration: 902, Loss: 0.5401906967163086\n",
      "Iteration: 903, Loss: 0.52482670545578\n",
      "Iteration: 904, Loss: 0.4971420466899872\n",
      "Iteration: 905, Loss: 0.5114484429359436\n",
      "Iteration: 906, Loss: 0.5694497227668762\n",
      "Iteration: 907, Loss: 0.5711380839347839\n",
      "Iteration: 908, Loss: 0.6019241213798523\n",
      "Iteration: 909, Loss: 0.5617361664772034\n",
      "Iteration: 910, Loss: 0.5404070019721985\n",
      "Iteration: 911, Loss: 0.5775209069252014\n",
      "Iteration: 912, Loss: 0.47362908720970154\n",
      "Iteration: 913, Loss: 0.5281186699867249\n",
      "Iteration: 914, Loss: 0.5919830203056335\n",
      "Iteration: 915, Loss: 0.5763564705848694\n",
      "Iteration: 916, Loss: 0.5214780569076538\n",
      "Iteration: 917, Loss: 0.5032216310501099\n",
      "Iteration: 918, Loss: 0.536543607711792\n",
      "Iteration: 919, Loss: 0.5645903944969177\n",
      "Iteration: 920, Loss: 0.5428927540779114\n",
      "Iteration: 921, Loss: 0.5391648411750793\n",
      "Iteration: 922, Loss: 0.6168938279151917\n",
      "Iteration: 923, Loss: 0.4340725839138031\n",
      "Iteration: 924, Loss: 0.5539829134941101\n",
      "Iteration: 925, Loss: 0.5207981467247009\n",
      "Iteration: 926, Loss: 0.5153815150260925\n",
      "Iteration: 927, Loss: 0.5650297999382019\n",
      "Iteration: 928, Loss: 0.5555879473686218\n",
      "Iteration: 929, Loss: 0.5458815097808838\n",
      "Iteration: 930, Loss: 0.5204436779022217\n",
      "Iteration: 931, Loss: 0.5334434509277344\n",
      "Iteration: 932, Loss: 0.5333426594734192\n",
      "Iteration: 933, Loss: 0.5118272304534912\n",
      "Iteration: 934, Loss: 0.5301149487495422\n",
      "Iteration: 935, Loss: 0.5715219974517822\n",
      "Iteration: 936, Loss: 0.5017516613006592\n",
      "Iteration: 937, Loss: 0.5608293414115906\n",
      "Iteration: 938, Loss: 0.5141357779502869\n",
      "Iteration: 939, Loss: 0.5353830456733704\n",
      "Iteration: 940, Loss: 0.5495185256004333\n",
      "Iteration: 941, Loss: 0.5507481694221497\n",
      "Iteration: 942, Loss: 0.5009008049964905\n",
      "Iteration: 943, Loss: 0.5368517637252808\n",
      "Iteration: 944, Loss: 0.5382052659988403\n",
      "Iteration: 945, Loss: 0.565014660358429\n",
      "Iteration: 946, Loss: 0.4919086694717407\n",
      "Iteration: 947, Loss: 0.5221477746963501\n",
      "Iteration: 948, Loss: 0.5948571562767029\n",
      "Iteration: 949, Loss: 0.6030338406562805\n",
      "Iteration: 950, Loss: 0.5027958750724792\n",
      "Iteration: 951, Loss: 0.5283777713775635\n",
      "Iteration: 952, Loss: 0.5406755208969116\n",
      "Iteration: 953, Loss: 0.5155906081199646\n",
      "Iteration: 954, Loss: 0.5167605876922607\n",
      "Iteration: 955, Loss: 0.5770145654678345\n",
      "Iteration: 956, Loss: 0.5932372212409973\n",
      "Iteration: 957, Loss: 0.5087123513221741\n",
      "Iteration: 958, Loss: 0.5630495548248291\n",
      "Iteration: 959, Loss: 0.488735169172287\n",
      "Iteration: 960, Loss: 0.6024072170257568\n",
      "Iteration: 961, Loss: 0.5537174940109253\n",
      "Iteration: 962, Loss: 0.509019672870636\n",
      "Iteration: 963, Loss: 0.4532177150249481\n",
      "Iteration: 964, Loss: 0.5848844647407532\n",
      "Iteration: 965, Loss: 0.5279797911643982\n",
      "Iteration: 966, Loss: 0.5247589349746704\n",
      "Iteration: 967, Loss: 0.605166494846344\n",
      "Iteration: 968, Loss: 0.5024762153625488\n",
      "Iteration: 969, Loss: 0.6023717522621155\n",
      "Iteration: 970, Loss: 0.45371994376182556\n",
      "Iteration: 971, Loss: 0.5528175830841064\n",
      "Iteration: 972, Loss: 0.5478543043136597\n",
      "Iteration: 973, Loss: 0.4973627030849457\n",
      "Iteration: 974, Loss: 0.6020776629447937\n",
      "Iteration: 975, Loss: 0.48980799317359924\n",
      "Iteration: 976, Loss: 0.5824559330940247\n",
      "Iteration: 977, Loss: 0.5032934546470642\n",
      "Iteration: 978, Loss: 0.506253182888031\n",
      "Iteration: 979, Loss: 0.6115705370903015\n",
      "Iteration: 980, Loss: 0.5765791535377502\n",
      "Iteration: 981, Loss: 0.5664085745811462\n",
      "Iteration: 982, Loss: 0.48967525362968445\n",
      "Iteration: 983, Loss: 0.570442795753479\n",
      "Iteration: 984, Loss: 0.5329204201698303\n",
      "Iteration: 985, Loss: 0.46602392196655273\n",
      "Iteration: 986, Loss: 0.552308201789856\n",
      "Iteration: 987, Loss: 0.5301182270050049\n",
      "Iteration: 988, Loss: 0.5792970657348633\n",
      "Iteration: 989, Loss: 0.4946455955505371\n",
      "Iteration: 990, Loss: 0.5734131336212158\n",
      "Iteration: 991, Loss: 0.5585337281227112\n",
      "Iteration: 992, Loss: 0.5261366367340088\n",
      "Iteration: 993, Loss: 0.5012189149856567\n",
      "Iteration: 994, Loss: 0.5366183519363403\n",
      "Iteration: 995, Loss: 0.5897681713104248\n",
      "Iteration: 996, Loss: 0.5265284776687622\n",
      "Iteration: 997, Loss: 0.5683354735374451\n",
      "Iteration: 998, Loss: 0.5791175961494446\n",
      "Iteration: 999, Loss: 0.5403497815132141\n",
      "Iteration: 1000, Loss: 0.47268441319465637\n",
      "Iteration: 1001, Loss: 0.5605310797691345\n",
      "Iteration: 1002, Loss: 0.523449182510376\n",
      "Iteration: 1003, Loss: 0.5444909930229187\n",
      "Iteration: 1004, Loss: 0.5209125876426697\n",
      "Iteration: 1005, Loss: 0.5742924213409424\n",
      "Iteration: 1006, Loss: 0.4924338161945343\n",
      "Iteration: 1007, Loss: 0.5645750164985657\n",
      "Iteration: 1008, Loss: 0.6050474643707275\n",
      "Iteration: 1009, Loss: 0.5108705163002014\n",
      "Iteration: 1010, Loss: 0.5333483219146729\n",
      "Iteration: 1011, Loss: 0.5558237433433533\n",
      "Iteration: 1012, Loss: 0.5667373538017273\n",
      "Iteration: 1013, Loss: 0.5462424159049988\n",
      "Iteration: 1014, Loss: 0.4877646565437317\n",
      "Iteration: 1015, Loss: 0.5746859908103943\n",
      "Iteration: 1016, Loss: 0.5413578748703003\n",
      "Iteration: 1017, Loss: 0.49305397272109985\n",
      "Iteration: 1018, Loss: 0.5318285226821899\n",
      "Iteration: 1019, Loss: 0.5998022556304932\n",
      "Iteration: 1020, Loss: 0.5694907307624817\n",
      "Iteration: 1021, Loss: 0.5156132578849792\n",
      "Iteration: 1022, Loss: 0.4959056079387665\n",
      "Iteration: 1023, Loss: 0.5536826252937317\n",
      "Iteration: 1024, Loss: 0.6214007139205933\n",
      "Iteration: 1025, Loss: 0.5066276788711548\n",
      "Iteration: 1026, Loss: 0.5923016667366028\n",
      "Iteration: 1027, Loss: 0.4941607415676117\n",
      "Iteration: 1028, Loss: 0.5038285851478577\n",
      "Iteration: 1029, Loss: 0.5908946394920349\n",
      "Iteration: 1030, Loss: 0.5550840497016907\n",
      "Iteration: 1031, Loss: 0.4734891355037689\n",
      "Iteration: 1032, Loss: 0.553693950176239\n",
      "Iteration: 1033, Loss: 0.5806592106819153\n",
      "Iteration: 1034, Loss: 0.5371330976486206\n",
      "Iteration: 1035, Loss: 0.560080885887146\n",
      "Iteration: 1036, Loss: 0.4748815894126892\n",
      "Iteration: 1037, Loss: 0.6125812530517578\n",
      "Iteration: 1038, Loss: 0.5241632461547852\n",
      "Iteration: 1039, Loss: 0.49094629287719727\n",
      "Iteration: 1040, Loss: 0.523038387298584\n",
      "Iteration: 1041, Loss: 0.4627965986728668\n",
      "Iteration: 1042, Loss: 0.5743557214736938\n",
      "Iteration: 1043, Loss: 0.6057564616203308\n",
      "Iteration: 1044, Loss: 0.47592034935951233\n",
      "Iteration: 1045, Loss: 0.4843209981918335\n",
      "Iteration: 1046, Loss: 0.5266924500465393\n",
      "Iteration: 1047, Loss: 0.5830562114715576\n",
      "Iteration: 1048, Loss: 0.5822821855545044\n",
      "Iteration: 1049, Loss: 0.4988390803337097\n",
      "Iteration: 1050, Loss: 0.5289170145988464\n",
      "Iteration: 1051, Loss: 0.5919846892356873\n",
      "Iteration: 1052, Loss: 0.5555694699287415\n",
      "Iteration: 1053, Loss: 0.5379610061645508\n",
      "Iteration: 1054, Loss: 0.48201724886894226\n",
      "Iteration: 1055, Loss: 0.5587915778160095\n",
      "Iteration: 1056, Loss: 0.5836165547370911\n",
      "Iteration: 1057, Loss: 0.5869321823120117\n",
      "Iteration: 1058, Loss: 0.5223479270935059\n",
      "Iteration: 1059, Loss: 0.5584004521369934\n",
      "Iteration: 1060, Loss: 0.4653382897377014\n",
      "Iteration: 1061, Loss: 0.5118076205253601\n",
      "Iteration: 1062, Loss: 0.530785858631134\n",
      "Iteration: 1063, Loss: 0.563667356967926\n",
      "Iteration: 1064, Loss: 0.580955445766449\n",
      "Iteration: 1065, Loss: 0.5515586137771606\n",
      "Iteration: 1066, Loss: 0.5082750916481018\n",
      "Iteration: 1067, Loss: 0.554245114326477\n",
      "Iteration: 1068, Loss: 0.5676227807998657\n",
      "Iteration: 1069, Loss: 0.5606116652488708\n",
      "Iteration: 1070, Loss: 0.49184557795524597\n",
      "Iteration: 1071, Loss: 0.5457732081413269\n",
      "Iteration: 1072, Loss: 0.5370681285858154\n",
      "Iteration: 1073, Loss: 0.5128902196884155\n",
      "Iteration: 1074, Loss: 0.5013454556465149\n",
      "Iteration: 1075, Loss: 0.594216525554657\n",
      "Iteration: 1076, Loss: 0.5430330634117126\n",
      "Iteration: 1077, Loss: 0.5547886490821838\n",
      "Iteration: 1078, Loss: 0.5664722323417664\n",
      "Iteration: 1079, Loss: 0.44930580258369446\n",
      "Iteration: 1080, Loss: 0.5195478200912476\n",
      "Iteration: 1081, Loss: 0.5300325155258179\n",
      "Iteration: 1082, Loss: 0.5750133991241455\n",
      "Iteration: 1083, Loss: 0.5030843019485474\n",
      "Iteration: 1084, Loss: 0.5647162795066833\n",
      "Iteration: 1085, Loss: 0.5506220459938049\n",
      "Iteration: 1086, Loss: 0.5616476535797119\n",
      "Iteration: 1087, Loss: 0.4492437541484833\n",
      "Iteration: 1088, Loss: 0.5561113953590393\n",
      "Iteration: 1089, Loss: 0.5859739184379578\n",
      "Iteration: 1090, Loss: 0.46722614765167236\n",
      "Iteration: 1091, Loss: 0.5263450741767883\n",
      "Iteration: 1092, Loss: 0.5667711496353149\n",
      "Iteration: 1093, Loss: 0.5072240233421326\n",
      "Iteration: 1094, Loss: 0.5958693027496338\n",
      "Iteration: 1095, Loss: 0.5143731832504272\n",
      "Iteration: 1096, Loss: 0.5455133318901062\n",
      "Iteration: 1097, Loss: 0.6545867919921875\n",
      "Iteration: 1098, Loss: 0.5367485880851746\n",
      "Iteration: 1099, Loss: 0.45585039258003235\n",
      "Iteration: 1100, Loss: 0.5178487300872803\n",
      "Iteration: 1101, Loss: 0.5061922669410706\n",
      "Iteration: 1102, Loss: 0.573633074760437\n",
      "Iteration: 1103, Loss: 0.5668752789497375\n",
      "Iteration: 1104, Loss: 0.5080679059028625\n",
      "Iteration: 1105, Loss: 0.5678122043609619\n",
      "Iteration: 1106, Loss: 0.6123685240745544\n",
      "Iteration: 1107, Loss: 0.562740683555603\n",
      "Iteration: 1108, Loss: 0.44049909710884094\n",
      "Iteration: 1109, Loss: 0.5448447465896606\n",
      "Iteration: 1110, Loss: 0.5923553109169006\n",
      "Iteration: 1111, Loss: 0.5453160405158997\n",
      "Iteration: 1112, Loss: 0.4958883821964264\n",
      "Iteration: 1113, Loss: 0.5925808548927307\n",
      "Iteration: 1114, Loss: 0.5657479763031006\n",
      "Iteration: 1115, Loss: 0.44069939851760864\n",
      "Iteration: 1116, Loss: 0.5435653924942017\n",
      "Iteration: 1117, Loss: 0.5642513632774353\n",
      "Iteration: 1118, Loss: 0.45953091979026794\n",
      "Iteration: 1119, Loss: 0.5735872983932495\n",
      "Iteration: 1120, Loss: 0.5269860029220581\n",
      "Iteration: 1121, Loss: 0.5706225037574768\n",
      "Iteration: 1122, Loss: 0.45765572786331177\n",
      "Iteration: 1123, Loss: 0.5399309992790222\n",
      "Iteration: 1124, Loss: 0.5872721076011658\n",
      "Iteration: 1125, Loss: 0.48086556792259216\n",
      "Iteration: 1126, Loss: 0.5655661821365356\n",
      "Iteration: 1127, Loss: 0.577483057975769\n",
      "Iteration: 1128, Loss: 0.5356982946395874\n",
      "Iteration: 1129, Loss: 0.5146549940109253\n",
      "Iteration: 1130, Loss: 0.5745936036109924\n",
      "Iteration: 1131, Loss: 0.4988214671611786\n",
      "Iteration: 1132, Loss: 0.5371859669685364\n",
      "Iteration: 1133, Loss: 0.5052200555801392\n",
      "Iteration: 1134, Loss: 0.5235046744346619\n",
      "Iteration: 1135, Loss: 0.561122477054596\n",
      "Iteration: 1136, Loss: 0.50188148021698\n",
      "Iteration: 1137, Loss: 0.4619942307472229\n",
      "Iteration: 1138, Loss: 0.5865073800086975\n",
      "Iteration: 1139, Loss: 0.5318049788475037\n",
      "Iteration: 1140, Loss: 0.553561270236969\n",
      "Iteration: 1141, Loss: 0.473906546831131\n",
      "Iteration: 1142, Loss: 0.5061043500900269\n",
      "Iteration: 1143, Loss: 0.6300705075263977\n",
      "Iteration: 1144, Loss: 0.5224741101264954\n",
      "Iteration: 1145, Loss: 0.5481895208358765\n",
      "Iteration: 1146, Loss: 0.5114139318466187\n",
      "Iteration: 1147, Loss: 0.47687992453575134\n",
      "Iteration: 1148, Loss: 0.6265324950218201\n",
      "Iteration: 1149, Loss: 0.5505290627479553\n",
      "Iteration: 1150, Loss: 0.5297461152076721\n",
      "Iteration: 1151, Loss: 0.5028799176216125\n",
      "Iteration: 1152, Loss: 0.5558338761329651\n",
      "Iteration: 1153, Loss: 0.4991326630115509\n",
      "Iteration: 1154, Loss: 0.59354567527771\n",
      "Iteration: 1155, Loss: 0.5866062641143799\n",
      "Iteration: 1156, Loss: 0.5042254328727722\n",
      "Iteration: 1157, Loss: 0.4436120390892029\n",
      "Iteration: 1158, Loss: 0.5909183025360107\n",
      "Iteration: 1159, Loss: 0.6192578077316284\n",
      "Iteration: 1160, Loss: 0.5221442580223083\n",
      "Iteration: 1161, Loss: 0.5268588066101074\n",
      "Iteration: 1162, Loss: 0.49619561433792114\n",
      "Iteration: 1163, Loss: 0.5358465313911438\n",
      "Iteration: 1164, Loss: 0.5546602606773376\n",
      "Iteration: 1165, Loss: 0.46240752935409546\n",
      "Iteration: 1166, Loss: 0.5574091076850891\n",
      "Iteration: 1167, Loss: 0.5734874606132507\n",
      "Iteration: 1168, Loss: 0.4840036928653717\n",
      "Iteration: 1169, Loss: 0.5380887985229492\n",
      "Iteration: 1170, Loss: 0.5448101162910461\n",
      "Iteration: 1171, Loss: 0.4899718463420868\n",
      "Iteration: 1172, Loss: 0.5586047768592834\n",
      "Iteration: 1173, Loss: 0.5378105640411377\n",
      "Iteration: 1174, Loss: 0.5499249696731567\n",
      "Iteration: 1175, Loss: 0.5294022560119629\n",
      "Iteration: 1176, Loss: 0.5055723190307617\n",
      "Iteration: 1177, Loss: 0.576045572757721\n",
      "Iteration: 1178, Loss: 0.43283188343048096\n",
      "Iteration: 1179, Loss: 0.5940420031547546\n",
      "Iteration: 1180, Loss: 0.5090750455856323\n",
      "Iteration: 1181, Loss: 0.5735614895820618\n",
      "Iteration: 1182, Loss: 0.49339786171913147\n",
      "Iteration: 1183, Loss: 0.5457733273506165\n",
      "Iteration: 1184, Loss: 0.4712676405906677\n",
      "Iteration: 1185, Loss: 0.5534077286720276\n",
      "Iteration: 1186, Loss: 0.5003973245620728\n",
      "Iteration: 1187, Loss: 0.577197253704071\n",
      "Iteration: 1188, Loss: 0.47979018092155457\n",
      "Iteration: 1189, Loss: 0.5314649343490601\n",
      "Iteration: 1190, Loss: 0.5678932070732117\n",
      "Iteration: 1191, Loss: 0.5016272664070129\n",
      "Iteration: 1192, Loss: 0.5296381711959839\n",
      "Iteration: 1193, Loss: 0.48934170603752136\n",
      "Iteration: 1194, Loss: 0.5304261445999146\n",
      "Iteration: 1195, Loss: 0.5848873853683472\n",
      "Iteration: 1196, Loss: 0.5121342539787292\n",
      "Iteration: 1197, Loss: 0.5077491402626038\n",
      "Iteration: 1198, Loss: 0.5045722723007202\n",
      "Iteration: 1199, Loss: 0.5133551359176636\n",
      "Iteration: 1200, Loss: 0.557690441608429\n",
      "Iteration: 1201, Loss: 0.5402401685714722\n",
      "Iteration: 1202, Loss: 0.5502131581306458\n",
      "Iteration: 1203, Loss: 0.55008864402771\n",
      "Iteration: 1204, Loss: 0.4785529375076294\n",
      "Iteration: 1205, Loss: 0.46279191970825195\n",
      "Iteration: 1206, Loss: 0.5342512726783752\n",
      "Iteration: 1207, Loss: 0.6231537461280823\n",
      "Iteration: 1208, Loss: 0.5162937045097351\n",
      "Iteration: 1209, Loss: 0.6141765713691711\n",
      "Iteration: 1210, Loss: 0.5072365999221802\n",
      "Iteration: 1211, Loss: 0.5271807909011841\n",
      "Iteration: 1212, Loss: 0.49312081933021545\n",
      "Iteration: 1213, Loss: 0.5671205520629883\n",
      "Iteration: 1214, Loss: 0.522314727306366\n",
      "Iteration: 1215, Loss: 0.5736811757087708\n",
      "Iteration: 1216, Loss: 0.5009008646011353\n",
      "Iteration: 1217, Loss: 0.5827549695968628\n",
      "Iteration: 1218, Loss: 0.5536391139030457\n",
      "Iteration: 1219, Loss: 0.520635724067688\n",
      "Iteration: 1220, Loss: 0.5177487730979919\n",
      "Iteration: 1221, Loss: 0.5674352049827576\n",
      "Iteration: 1222, Loss: 0.5295316576957703\n",
      "Iteration: 1223, Loss: 0.5672593712806702\n",
      "Iteration: 1224, Loss: 0.49012166261672974\n",
      "Iteration: 1225, Loss: 0.5895439982414246\n",
      "Iteration: 1226, Loss: 0.551487386226654\n",
      "Iteration: 1227, Loss: 0.5301327109336853\n",
      "Iteration: 1228, Loss: 0.5191711187362671\n",
      "Iteration: 1229, Loss: 0.5977453589439392\n",
      "Iteration: 1230, Loss: 0.4639640152454376\n",
      "Iteration: 1231, Loss: 0.4917818605899811\n",
      "Iteration: 1232, Loss: 0.5497961044311523\n",
      "Iteration: 1233, Loss: 0.5214444994926453\n",
      "Iteration: 1234, Loss: 0.5405922532081604\n",
      "Iteration: 1235, Loss: 0.5828922986984253\n",
      "Iteration: 1236, Loss: 0.5330013632774353\n",
      "Iteration: 1237, Loss: 0.49664509296417236\n",
      "Iteration: 1238, Loss: 0.5551358461380005\n",
      "Iteration: 1239, Loss: 0.5734363198280334\n",
      "Iteration: 1240, Loss: 0.5342980623245239\n",
      "Iteration: 1241, Loss: 0.5056719779968262\n",
      "Iteration: 1242, Loss: 0.5364481806755066\n",
      "Iteration: 1243, Loss: 0.5507422089576721\n",
      "Iteration: 1244, Loss: 0.5736417174339294\n",
      "Iteration: 1245, Loss: 0.5523546934127808\n",
      "Iteration: 1246, Loss: 0.5302866697311401\n",
      "Iteration: 1247, Loss: 0.5108423829078674\n",
      "Iteration: 1248, Loss: 0.5357211232185364\n",
      "Iteration: 1249, Loss: 0.5035103559494019\n",
      "Iteration: 1250, Loss: 0.6155188679695129\n",
      "Iteration: 1251, Loss: 0.4930068254470825\n",
      "Iteration: 1252, Loss: 0.5054107904434204\n",
      "Iteration: 1253, Loss: 0.5591275095939636\n",
      "Iteration: 1254, Loss: 0.5637816190719604\n",
      "Iteration: 1255, Loss: 0.5593031048774719\n",
      "Iteration: 1256, Loss: 0.4711325466632843\n",
      "Iteration: 1257, Loss: 0.49287188053131104\n",
      "Iteration: 1258, Loss: 0.5464228391647339\n",
      "Iteration: 1259, Loss: 0.5935217142105103\n",
      "Iteration: 1260, Loss: 0.5315116047859192\n",
      "Iteration: 1261, Loss: 0.47604382038116455\n",
      "Iteration: 1262, Loss: 0.5131528377532959\n",
      "Iteration: 1263, Loss: 0.5428663492202759\n",
      "Iteration: 1264, Loss: 0.5907962322235107\n",
      "Iteration: 1265, Loss: 0.5495613813400269\n",
      "Iteration: 1266, Loss: 0.5188440680503845\n",
      "Iteration: 1267, Loss: 0.5746503472328186\n",
      "Iteration: 1268, Loss: 0.5013530254364014\n",
      "Iteration: 1269, Loss: 0.5273877382278442\n",
      "Iteration: 1270, Loss: 0.5902141332626343\n",
      "Iteration: 1271, Loss: 0.5133486390113831\n",
      "Iteration: 1272, Loss: 0.5221899151802063\n",
      "Iteration: 1273, Loss: 0.554699182510376\n",
      "Iteration: 1274, Loss: 0.48430517315864563\n",
      "Iteration: 1275, Loss: 0.559753954410553\n",
      "Iteration: 1276, Loss: 0.5549429059028625\n",
      "Iteration: 1277, Loss: 0.4648244082927704\n",
      "Iteration: 1278, Loss: 0.6498375535011292\n",
      "Iteration: 1279, Loss: 0.5472328066825867\n",
      "Iteration: 1280, Loss: 0.5079407095909119\n",
      "Iteration: 1281, Loss: 0.5517500042915344\n",
      "Iteration: 1282, Loss: 0.534223735332489\n",
      "Iteration: 1283, Loss: 0.5248962640762329\n",
      "Iteration: 1284, Loss: 0.5644239187240601\n",
      "Iteration: 1285, Loss: 0.49815839529037476\n",
      "Iteration: 1286, Loss: 0.5789425373077393\n",
      "Iteration: 1287, Loss: 0.5248501300811768\n",
      "Iteration: 1288, Loss: 0.5331494808197021\n",
      "Iteration: 1289, Loss: 0.6369102597236633\n",
      "Iteration: 1290, Loss: 0.5454203486442566\n",
      "Iteration: 1291, Loss: 0.44192206859588623\n",
      "Iteration: 1292, Loss: 0.557761013507843\n",
      "Iteration: 1293, Loss: 0.5084879398345947\n",
      "Iteration: 1294, Loss: 0.559965193271637\n",
      "Iteration: 1295, Loss: 0.5334442853927612\n",
      "Iteration: 1296, Loss: 0.5249050855636597\n",
      "Iteration: 1297, Loss: 0.5182479619979858\n",
      "Iteration: 1298, Loss: 0.58545982837677\n",
      "Iteration: 1299, Loss: 0.522228479385376\n",
      "Iteration: 1300, Loss: 0.503924548625946\n",
      "Iteration: 1301, Loss: 0.584916889667511\n",
      "Iteration: 1302, Loss: 0.4929129183292389\n",
      "Iteration: 1303, Loss: 0.5115614533424377\n",
      "Iteration: 1304, Loss: 0.594880998134613\n",
      "Iteration: 1305, Loss: 0.5237225890159607\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/esteb/Desktop/PCC/denoiser.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/esteb/Desktop/PCC/denoiser.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update model parameters.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/esteb/Desktop/PCC/denoiser.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIteration: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i, loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/esteb/Desktop/PCC/denoiser.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m train()\n",
      "\u001b[1;32m/home/esteb/Desktop/PCC/denoiser.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/esteb/Desktop/PCC/denoiser.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m e_theta, e_rand \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mpos, data\u001b[39m.\u001b[39mbatch)  \u001b[39m# Forward pass.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/esteb/Desktop/PCC/denoiser.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(e_theta, e_rand)  \u001b[39m# Loss computation.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/esteb/Desktop/PCC/denoiser.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Backward pass.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/esteb/Desktop/PCC/denoiser.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update model parameters.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/esteb/Desktop/PCC/denoiser.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIteration: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/.conda/envs/pfc/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.conda/envs/pfc/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(zdim=train_dataset.num_classes, num_steps=1000, beta_1=1e-4, beta_T=0.02)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()  # Define loss criterion.\n",
    "\n",
    "def train():\n",
    "    \n",
    "    for i, data in enumerate(get_data_iterator(train_loader), 1):\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        model.train() \n",
    "        \n",
    "        e_theta, e_rand = model(data.pos, data.batch)  # Forward pass.\n",
    "        loss = criterion(e_theta, e_rand)  # Loss computation.\n",
    "        \n",
    "        loss.backward()  # Backward pass.\n",
    "        optimizer.step()  # Update model parameters.\n",
    "\n",
    "        print(\"Iteration: {}, Loss: {}\".format(i, loss.item()))\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = VarianceSchedule(100, 1e-4, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 12, 24, 72, 92, 81, 93, 19, 88, 26]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0905, 0.0441, 0.0659, 0.1186, 0.1347, 0.1261, 0.1354, 0.0578, 0.1316,\n",
       "        0.0689])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = schedule.uniform_sample_t(10)\n",
    "print(steps)\n",
    "schedule.get_sigmas(torch.tensor(steps), 0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
